{"meta":{"title":"HomeLabProject","subtitle":"","description":"家里开IDC，但是什么都不会，一问三不知","author":"Channing He","url":"https://homelabproject.cc","root":"/"},"pages":[{"title":"","date":"2025-05-29T14:39:49.610Z","updated":"2025-05-29T14:39:49.610Z","comments":true,"path":"custom.css","permalink":"https://homelabproject.cc/custom.css","excerpt":"","text":":root{--gutter:25px;--radius:13px;--color-primary:#1c6c36;--color2:#ff761e;--color3:#ffb900;--color4:#33d57a;--color5:#00dbff;--color6:#1a98ff;--color7:#9090ff;--color-primary-bg:rgba(20, 103, 10, 0.15);--color2-bg:rgba(255,118,30,0.15);--color3-bg:rgba(255,185,0,0.15);--color4-bg:rgba(51,213,122,0.15);--color5-bg:rgba(0,219,255,0.15);--color6-bg:rgba(26,152,255,0.15);--color7-bg:rgba(144,144,255,0.15);--color-shadow:rgba(161, 177, 204, 0.4)}.nexmoe-post-footer{background-color:rgba(16,16,16,0)}.nexmoe-post-copyright{background-color:#ffffff78}article ul li{line-height:1.5rem!important}"}],"posts":[{"title":"Hexo Blog方案","slug":"Infra/Hexo Blog方案","date":"2025-05-29T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"Infra/Hexo Blog方案/","permalink":"https://homelabproject.cc/Infra/Hexo%20Blog%E6%96%B9%E6%A1%88/","excerpt":"","text":"WIP","categories":[],"tags":[{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"}]},{"title":"自建 APT 镜像源 Tunasync 定时 多线程","slug":"Infra/自建 APT 镜像源 Tunasync 定时 多线程","date":"2025-05-27T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"Infra/自建 APT 镜像源 Tunasync 定时 多线程/","permalink":"https://homelabproject.cc/Infra/%E8%87%AA%E5%BB%BA%20APT%20%E9%95%9C%E5%83%8F%E6%BA%90%20Tunasync%20%E5%AE%9A%E6%97%B6%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"源码 Github已经发布，README包含大部分说明 https://github.com/ChanningHe/tunasync-docker 介绍 家里的服务器越来越多，基础镜像比如Debian之类的需要一个镜像，方便本地快速拉取。 ProxmoxVE的非订阅源用户人数多，还是用起来不是那么快，加上集群机器多。 TrueNAS编译的时候严重依赖他们的APT源，他们的APT源又奇慢，加载只有几百KB/s，换完本地后编译从2h+变成1h+，感知还是明显的。 Tailscale源还是国内访问十分慢的源，用的机器镜像拉不下来，很多校园源又不做商业软件的源。 自建源地址 https://mirrors.homelabproject.cc/ （只是做个示例，在Cloudflare CDN前只是公开图一乐）","categories":[],"tags":[{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"},{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"},{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"}]},{"title":"TrueNAS 25.04 Docker最佳实践（文件权限篇）","slug":"TrueNAS/TrueNAS 25.04  Docker最佳实践（文件权限UID篇）","date":"2025-05-24T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"TrueNAS/TrueNAS 25.04  Docker最佳实践（文件权限UID篇）/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%2025.04%20%20Docker%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90UID%E7%AF%87%EF%BC%89/","excerpt":"","text":"介绍 TrueNAS SCALE 24.10从K8s的支持转到了更好的维护的Docker（中间价需要调用的东西太多了），基本上告别SCALE这个词（这也就是25.04已经不叫SCALE的原因）。 Docker本身对于应用的部署难度是大幅降度的，但是Docker难度的降低其实是有明显代价的，就是权限问题。 这片文章目的在于分析当前Docker镜像带来的各种问题 虽然实在TrueNAS 25.04运行，但是理论在任何系统都通用，都可以作为docker运行的一个 目的 安全第一 因为有对外服务，要保证容器即使被逃逸也不会得到host的root权限 挂载目录权限统一 所有的docker容器应用最后都属于一个UID或者GID，方便权限管理 性能 逃不开的话题，虽然有nfs over rdma能降低挂给其他虚拟机损耗，但是本身zfs对于nvme性能的损耗已经十分大，能通过直接挂载目录而非网络挂载，在CPU，磁盘等方面的消耗也能减少。同时在host管理目录也更加方便。 问题 Docker Daemon 首先众所周知的Docker Daemon（守护进程）永远是root运行。 默认进程和UID Docker 默认的进程或者在容器内的能获得到的root权限就是宿主的root ，因为默认没有启用 user namespace remapping，所以如果容器逃逸了、或者被赋予了访问宿主资源的能力，它就能像宿主机 root 一样行动。 Docker出的解决方法 USER 选项：让容器内的进程直接都以指定UID运行，但是带来很大问题问题是，很多镜像会操作非挂载的系统目录，最简单的比如说/var/log/ 下创建log文件，都会因为非root无法创建，直接无法运行。 Env变量指定UID：这个就是更是阴间中的阴间。因为这个不是一个标准操作，每个镜像作者都有自己想法。 例子1 比如著名的linuxserver/的镜像，他特别贴心的让他们发布的每一个镜像都支持指定PUID和PGID，似乎这是一个能解决，但是你可以看到他们具体的实现方式。 在容器启动的是读取PUID env，非默认的情况下创建对应用户，然后对挂载目录执行lsiown https://github.com/linuxserver/docker-baseimage-ubuntu/blob/212ab2133eb21e4e4b536039c790201baf81fd4f/root/etc/s6-overlay/s6-rc.d/init-adduser/run#L53 123456789101112131415161718192021#... #... PUID=${PUID:-911} PGID=${PGID:-911} if [[ -z ${LSIO_READ_ONLY_FS} ]] &amp;&amp; [[ -z ${LSIO_NON_ROOT_USER} ]]; then USERHOME=$(grep abc /etc/passwd | cut -d &quot;:&quot; -f6) usermod -d &quot;/root&quot; abc groupmod -o -g &quot;${PGID}&quot; abc usermod -o -u &quot;${PUID}&quot; abc usermod -d &quot;${USERHOME}&quot; abc fi #... #... if [[ -z ${LSIO_READ_ONLY_FS} ]] &amp;&amp; [[ -z ${LSIO_NON_ROOT_USER} ]]; then lsiown abc:abc /app lsiown abc:abc /config lsiown abc:abc /defaults fi lsiown好像也是他们自己制作的脚本本质就是chown一下。 https://github.com/linuxserver/docker-mods/blob/cac9e7450a0698f19d750b67db61c4aa214d5290/lsiown.v1#L30 1/usr/bin/find &quot;${PATH[@]}&quot; &quot;${MAXDEPTH[@]}&quot; ! -xtype l \\( ! -group &quot;${GROUP}&quot; -o ! -user &quot;${USER}&quot; \\) -exec chown &quot;${OPTIONS[@]}&quot; &quot;${USER}&quot;:&quot;${GROUP}&quot; {} + || printf &quot;${ERROR}&quot; 只对Dockerfile里面定义的路径chown，对于媒体库这样挂载路径多样的情况，根本照顾不到 用root对挂载进来的“指定配置路径”进行了chown，更改了所有者直接破坏原来的权限。 只有应用进程本身是UID是指定的UID，容器内本身还保留root权限，那么这个容器本身还是存在root用户并且可以调用，只解决文件权限问题，而且是破坏性解决 不是啊，说到底你还是能拿root啊（ 例子2 最近碰到一个更加奇怪（阴间）的镜像 —&gt; Gitea ，他官方提供了2个镜像，一个是basic的一个是rootless 先来看下面的basic，支持USER_UID指定，那么基本来说应该是linuxserver的镜像差不多的方式。 12345678910111213services: server: image: docker.gitea.com/gitea:1.23.8 restart: always environment: - USER_UID=1000 - USER_GID=1000 volumes: - ./data:/var/lib/gitea - ./config:/etc/gitea ports: - \"3000:3000\" - \"2222:2222\" 容器内本身还有root权限，通过entrypoint运行脚本，替换passwd文件里面git用户的uid，其实也都属于常规操作。 https://github.com/go-gitea/gitea/blob/688da55f543f82265cc7df2bd1cf2bce53188b7a/docker/root/usr/bin/entrypoint#L22 123456789if [ -n &quot;${USER_GID}&quot; ] &amp;&amp; [ &quot;${USER_GID}&quot; != &quot;`id -g ${USER}`&quot; ]; then sed -i -e &quot;s/^${USER}:\\([^:]*\\):[0-9]*/${USER}:\\1:${USER_GID}/&quot; /etc/group sed -i -e &quot;s/^${USER}:\\([^:]*\\):\\([0-9]*\\):[0-9]*/${USER}:\\1:\\2:${USER_GID}/&quot; /etc/passwd fi # Change UID for USER? if [ -n &quot;${USER_UID}&quot; ] &amp;&amp; [ &quot;${USER_UID}&quot; != &quot;`id -u ${USER}`&quot; ]; then sed -i -e &quot;s/^${USER}:\\([^:]*\\):[0-9]*:\\([0-9]*\\)/${USER}:\\1:${USER_UID}:\\2/&quot; /etc/passwd fi 那么rootless镜像呢，我看了很兴奋，终于有人做纯正的rootless镜像了，官网的compose事例如下，当时觉得奇怪，都rootless怎么没指定UID，手动加上了basic镜像里面的USER_UID环境变量，发现根本跑不起来，当时在试podman，以为是podman哪里没配对。 1234567891011121314version: &quot;2&quot; services: server: image: docker.gitea.com/gitea:1.23.8-rootless restart: always volumes: - ./data:/var/lib/gitea - ./config:/etc/gitea - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - &quot;3000:3000&quot; - &quot;2222:2222&quot; 然后好奇看了眼官方打包的Dockerfile。 https://github.com/go-gitea/gitea/blob/688da55f543f82265cc7df2bd1cf2bce53188b7a/Dockerfile.rootless#L77 123456# git:git USER 1000:1000 ENV GITEA_WORK_DIR=/var/lib/gitea ENV GITEA_CUSTOM=/var/lib/gitea/custom ENV GITEA_TEMP=/tmp/gitea ENV TMPDIR=/tmp/gitea 谁TMD的rootless镜像指定1000 uid不能更改用的？最后自己拉过来改了dockerfile然后编译 例子3 除了上面那些好歹你能指定一下，有些镜像，你真的不知道他们是什么UID镜像。 postgres这么一个常用镜像，默认uid是999，挂载目录进去因为权限限制的比较死，直接提示权限不够。 虽然postgres库下面提示，可以直接用user去指定uid运行，但是你需要去处理passwd文件。 [!NOTE] As of docker-library/postgres#253⁠, this image supports running as a (mostly) arbitrary user via --user on docker run. As of docker-library/postgres#1018⁠, this is also the case for the Alpine variants. The main caveat to note is that postgres doesn’t care what UID it runs as (as long as the owner of /var/lib/postgresql/data matches), but initdbdoes care (and needs the user to exist in /etc/passwd): 挂载目录没法处理权限这个问题，在k8s或者大型项目中都会使用卷挂载或者pvc之类的块存来避免权限问题，但是在家里挂载目录还是更好维护操作文件，并且好做快照恢复。 总结下来就是，每个容器镜像的实现方式根据开发者的喜好五花八门，层次不齐。但是最终还是因为docker本身没有一个很好且规范的解决方法。 最佳实践 原理 简单来说，把Docker塞进另外的非特权的系统级容器（incus或者lxc）中，通过idmaps 映射host机器的一个组，让容器内要使用的所有用户加入这个容器内的这个组，也就会被自动映射到host的组。另外incus和lxc这类容器默认的root用户也会被映射到主机非常大uid（65534+）的非root用户来确保安全性。 https://linuxcontainers.org/incus/docs/main/userns-idmap/ 那么，在incus容器中的debian，我们就可以随意运行docker容器即使这个应用是root 他获得挂载文件权限的映射路径是 UID:0(Docker) -&gt; UID:0 (IncusDebian) -&gt; GID:3004(IncusDebian) -&gt; GID:3004(TrueNAS Host) 那么即使Docker容器被突破，他能获取到的最大权限是TrueNAS UID:65534+ | GUID:65534+的进程权限和GID:3004的文件权限 具体操作 IDMaps 先添加一个idmaps，在TrueNAS GUI里面 Instance -&gt; Configuration -&gt; Map User/Group IDs, 转到Groups选项，添加你想要映射的组，在图里，我映射了名字rootless 的 GID:3004的图，所以我挂载的所有的路径都会在数据集里面添加rootless的权限。 PS：注意添加权限的时候，请直接添加为Owner Group（所有组），而不是使用NFS/SMB ACL权限的下的权限，并且勾选Apply Group以及递归，如果你要使用ACL权限，请使用POSIX ACL权限 Incus运行Debian以及Docker环境 安装Debian没什么技巧，直接在GUI选择debian安装，然后挂载你需要的路径就行了。 安装完成后，因为GUI还有很多选项没有开发，所以我们需要CLI配置一下 1sudo incus config edit [debian容器名] 你需要在config: 的里面添加添加这三行之后，保存退出（nano操作） 123security.nesting: &quot;true&quot; security.syscalls.intercept.mknod: &quot;true&quot; security.syscalls.intercept.setxattr: &quot;true&quot; 重启容器 1sudo incus restart [debian容器名] 进入容器安装Docker，注意你的网络环境 1234sudo incus shell [debian容器名] apt update &amp;&amp; apt install nano curl -y curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh 添加组，将root加入组rootless 1234groupadd -g 3004 rootless usermod -aG rootless root #重新登陆生效 su - root 然后你再去检查挂载路径，会发现root已经能正常访问。此时你已经能运行大部分容器了 各种容器的应对方法 可配置UID/GID的容器 可以考虑直接配置为UID=0 GID=3004基本上能运行。 不配配置，固定UID的容器 比如postgres这样以UID=999运行的容器，可以模仿root加入组的方式，先找到系统UID=999的应用，例如为这边是netdata 12root@Debian:~# cat /etc/passwd | grep 999 netdata:x:999:997::/var/lib/netdata:/bin/sh 然后让netdata加入rootless组，postgres就能正常运行了 1usermod -aG rootless netdata 多进程不同UID或者即使指定GID 还是没权限 部分容器镜像UID处理方式好像不正常，那么我需要进入这个docker容器，把容器内部的 /etc/group 文件复制出来，然后 ps aux查看进程的UID 在最后一行加上，即让容器的root以及进程ID加入rootless组（没错就是这么阴间，容器内的root有时候会继承不到组权限） 1rootless:x:3004:root,[各种进程ID] 然后改完的文件再在compose里面挂载回 容器内的 /etc/group 1234567services: server: #..... volumes: - ./fake-group-file:/etc/group #..... #..... 总结 总体下来，为了Docker安全性其实付出的维护量还是挺大的，当然如果你不是有需要暴露公网，且不在乎权限，你当然可以root一把嗦，节约自己的时间。 另外可能有人会提到Podman，在安全性上能让容器默认就映射root用户，100%避免了root进程的问题，网络方面也干干净净，但是Podman从历史来看变动太大了，从compose转换的支持到现在的quadlet的写法，以及听说要支持kube apply的yaml写法。让我在主力机器上使用没有很大动力。另外支持Podman的管理工具大部分都还停留在compose。兼容性差的要命，各种option不兼容。","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"},{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"},{"name":"Incus","slug":"Incus","permalink":"https://homelabproject.cc/tags/Incus/"}]},{"title":"Mellanox SN2700 风扇 改造","slug":"Others/Mellanox SN2700 风扇 改造","date":"2025-05-21T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"Others/Mellanox SN2700 风扇 改造/","permalink":"https://homelabproject.cc/Others/Mellanox%20SN2700%20%E9%A3%8E%E6%89%87%20%E6%94%B9%E9%80%A0/","excerpt":"","text":"几个月前在闲鱼刷到群里的流量王交换机SN2700，但是结尾是B，问卖家和看datasheet应该是个40G，但是网上搜了一下有人买过SN2100带B结果实际能跑100G。和卖家一顿砍价，5K出头，考虑到价格，符合垃圾佬不一定用得到但是一定要捡漏。 拿到手一看，确实能100G，实际测速也没问题。 剩下遇到的问题就是噪音，虽然SN2700也能想SX60xx系列用命令控制，但是还是会来回拉升，普遍在1w转，满载2w，放在一个小房间在外面也听得到，所以还是得改一下。 SN2700的风扇是模组化的，模组上有个8pin的母座，是sh1.0，建议淘宝买带线做好的，因为sh1.0的端子是真的压不出来。（另外淘宝sh1.0的胶壳普遍质量差，很容易导致脚被压歪，一定要直上直下使用） 线序如下图左到右： 12V ｜ 测速 ｜ PWM调速 ｜ GND ｜12V ｜ 测速 ｜ PWM调速 ｜ GND 改风扇的关键点： PWM调速 不能接，让风扇直接满速运行 2个测速脚一定要接 风扇转速大于 不等于5K转（低于系统最低阈值，但是不在乎报错是能用的） 风扇只能满速的话，导致4056风扇没得选，因为所有4056都是1-2w转满速，我这里的办法是选择单个4028或者2个4020风扇 单个风扇的话，推荐arctic 4028 6K，转速达标，性能可以。（不要买汕头扇子！永远不是贴标上的型号的翻新），做线的话，把2个测速脚都连风扇的一个脚上即可，其他脚正常链接。 2个4020的话，随便买吧，找个正品的6K左右的3线或者4线风扇，一定带测速，除了pwm调速不接其他练到对应脚就行。然后需要2个隔离柱，太近了因为不是反桨反而效果差，需要隔开点，我这里是3D打印的。（6-2.8-5.8/6-2.8-9.5/外经-内径-长度）每个模块每样2个。最后效果是这样 图一乐的是，我每样做了2个（ 擦电上机，转速都能识别到，但是低于5K的几把风扇过一会就会报转速过低，有空再换掉。 冬天室温很低，温度也没什么问题。夏天可能会高点，但是考虑到我估计连一半端口都用不了，压力也不大。","categories":[],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"https://homelabproject.cc/tags/Hardware/"}]},{"title":"TrueNAS 日志过多","slug":"TrueNAS/TrueNAS 日志过多 debug","date":"2025-04-19T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"TrueNAS/TrueNAS 日志过多 debug/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20%E6%97%A5%E5%BF%97%E8%BF%87%E5%A4%9A%20debug/","excerpt":"","text":"1journalctl -q --no-pager --output=json | jq -r &#39;._SYSTEMD_UNIT&#39; | sort | uniq -c | sort -nr 1cat /var/log/middlewared.log","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"TrueNAS  APP（docker）替换官方存储库（repo","slug":"TrueNAS/TrueNAS  APP（docker）替换官方存储库（repo）","date":"2025-02-16T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"TrueNAS/TrueNAS  APP（docker）替换官方存储库（repo）/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20%20APP%EF%BC%88docker%EF%BC%89%E6%9B%BF%E6%8D%A2%E5%AE%98%E6%96%B9%E5%AD%98%E5%82%A8%E5%BA%93%EF%BC%88repo%EF%BC%89/","excerpt":"","text":"首先官方存储库定义文件在 /usr/lib/python3/dist-packages/middlewared/plugins/catalog/utils.py 这是个不可读文件，所以我们需要用mount的方式 下面操作一律root操作 先创建一个放utils.py 替换文件的路径， 12cd /mnt/xxx/xxx mkdir -p ./middlewared-override/plugins/catalog/ 复制源文件 1cp /usr/lib/python3/dist-packages/middlewared/plugins/catalog/utils.py ./middlewared-override/plugins/catalog/utils.py 修改URL，你可以nano自己改，可以用下面的sed直接替换 1sed -i &#39;s|https://github.com/truenas/apps|https://[github 加速镜像]/truenas/apps|g&#39; ./middlewared-override/plugins/catalog/utils.py 用mount —bind强制覆盖 1mount --bind ./middlewared-override/plugins/catalog/utils.py /usr/lib/python3/dist-packages/middlewared/plugins/catalog/utils.py 最后重启中间价即可 1systemctl restart middlewared","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"Ros 静态路由 到 内网VPN网关慢","slug":"Others/RouterOS 静态路由 到 内网VPN网关慢","date":"2025-02-09T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"Others/RouterOS 静态路由 到 内网VPN网关慢/","permalink":"https://homelabproject.cc/Others/RouterOS%20%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1%20%E5%88%B0%20%E5%86%85%E7%BD%91VPN%E7%BD%91%E5%85%B3%E6%85%A2/","excerpt":"","text":"问题 RouterOS在静态路由 （VPN网段或者 异地网段） 到内网VPN网关时，会遇到体现是第一个响应很慢，但是加载后正常。 排难，iperf3测试带宽没问题，ping延迟也正常的情况。 原因 （懒得写太长，建议看下方链接） 大概就是本地发包转发给内网VPN网关出口后连接无法追踪后判定为”invalid”，通常在防火墙规则最后会有 1chain=forward action=drop connection-state=invalid 导致连接出现”invalid”后直接被drop掉了 解决 在chain=forward action=drop connection-state=invalid 前添加 1chain=forward action=accept connection-state=invalid,new src-address=192.168.2.0/24 dst-address=10.1.1.0/24 in-interface=lan-bridge 即可。 scr-address是你的lan地址，dst-address是你的想要的静态路由的网段（VPN网段或者 异地网段） 就是接受转发到静态路由的网段的”invalid”包 参考 https://forum.mikrotik.com/viewtopic.php?t=171177","categories":[],"tags":[{"name":"RouterOS","slug":"RouterOS","permalink":"https://homelabproject.cc/tags/RouterOS/"}]},{"title":"TrueNAS SCALE NVMe of (nvmetcli)教程","slug":"TrueNAS/TrueNAS SCALE NVMe of (nvmetcli)教程","date":"2025-01-18T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"TrueNAS/TrueNAS SCALE NVMe of (nvmetcli)教程/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20SCALE%20NVMe%20of%20(nvmetcli)%E6%95%99%E7%A8%8B/","excerpt":"","text":"更新 2025.05.29 NVME-OF将在25.10 官方支持 视频教程 https://www.bilibili.com/video/BV1AgrYY6EYn 前言 TrueNAS SCALE作为一个存储系统，在NVMe SSD价格越来越香，其实一直缺少高性能的共享存储方式。 对于自带的NFS来说，开启RDMA非常简单只需要给出rdma端口以及加载模块，配置十分简单。SMB direct的话SCALE的内核不支持，所以想要改起来就非常麻烦。 块存储在2025年只有ISCSI对于高负载的应用来说，显得就不够了，无论是tcp还是rdma的NVMeOF在各种方面都优于ISCSI，而且能显著降低CPU的利用率。 其实SCALE的企业版已经支持了NVMeOF 来源：https://www.starwindsoftware.com/blog/iscsi-vs-nvme-of-performance-comparison/ 以及其他的一些对比：https://kb.blockbridge.com/technote/proxmox-iscsi-vs-nvmetcp/# 所以配置NVMeOf迫在眉睫（不是。 入门（ 对于TrueNAS SCALE，因为更新会丢失命令行所有的内容，所以使用nvmetcli 保存配置文件，在更新后只需要加载python环境，pip安装依赖，执行nvmetcli restore config.json 即可恢复nvmeof。 具体操作 配置环境 其实已经有项目写好了脚本，我们就不重复造轮子，简单论述使用方法 https://github.com/democratic-csi/democratic-csi/blob/master/contrib/scale-nvmet-start.sh 1234#切换root su - root #输入密码 curl -o /mnt/池名字/你放脚本/数据集的名字/scale-nvmet-start.sh https://raw.githubusercontent.com/democratic-csi/democratic-csi/refs/heads/master/contrib/scale-nvmet-start.sh 作为例子我这里的脚本放在CPool/myscripts ,后面都要替换我这里的路径 1234#切换root su - root #输入密码 curl -o /mnt/CPool/myscripts/scale-nvmet-start.sh https://raw.githubusercontent.com/democratic-csi/democratic-csi/refs/heads/master/contrib/scale-nvmet-start.sh 然后给予执行权限 1chmod +x /mnt/CPool/myscripts/scale-nvmet-start.sh 需要注意的是这个脚本默认安装最新版本的依赖~~configshell_fb~~ ，这会导致报错，需要指定版本1.1.30 也就是修改这个脚本~~pip install configshell_fb~~ 到~~pip install configshell_fb==1.1.30~~ 然后执行脚本 忽略上面的话，提交修复的PR以及合并，默认就是安装这个版本的了。 1bash /mnt/CPool/myscripts/scale-nvmet-start.sh 如果你网络没问题，那么这个脚本会帮你完成一切的配置 nvmetctl配置NVMeOF 执行下面的命令进入nvmetcli命令行，注意替换CPool/myscripts 1/mnt/CPool/myscripts/nvmet-venv/bin/python /root/.local/bin/nvmetcli 首先要配置一个**subsystems 和** namespaces 1234subsystems/ create nqn=你爱叫什么叫什么 #然后爱叫，我这里叫testof #创建namespaces subsystems/testof/namespaces create nsid=1 然后你可以输入ls查看现在的配置 123456789/&gt; ls o- / ......................................................................................................................... [...] o- hosts ................................................................................................................... [...] o- ports ................................................................................................................... [...] o- subsystems .............................................................................................................. [...] o- testof .............................................................. [version=1.3, allow_any=0, serial=2cb2b6bc4b46653b7680] o- allowed_hosts ....................................................................................................... [...] o- namespaces .......................................................................................................... [...] o- 1 ........................................... [path=(null), uuid=d88dc614-c22d-4942-8a97-5108f210aa1f, grpid=1, disabled] 这里就要修改这个namespaces的path到你的zvol路径,我这里在创建了一个在 ttt池config数据集下的叫nvme-zvol的zvol，那么路径就是固定的**/dev/zvol**+前面的那些，如下 12345/subsystems/testof/namespaces/1 set device path=/dev/zvol/ttt/config/nvme-zvol #同时enable这个namespaces /subsystems/testof/namespaces/1 enable #给testof这个subsystems配置允许任何链接用于测试 /subsystems/testof/ set attr allow_any_host=1 到这里目标配置完毕了。 还要配置一下本机port 1234567#创建一个port /ports create portid=1 #配置addr和param，替换下面ip到你的本机ip /ports/1/ set addr adrfam=ipv4 traddr=192.168.2.140 trtype=rdma trsvcid=4420 /ports/1/ set param inline_data_size=4096 #启动subsystem，没有任何报错就是成功了。如果有报错，看内核日志能看到详细错误 /ports/1/subsystems create testof 最后ls一下 123456789101112131415/&gt; ls o- / .................................................................................... [...] o- hosts .............................................................................. [...] o- ports .............................................................................. [...] | o- 1 ............... [trtype=rdma, traddr=192.168.2.140, trsvcid=4420, inline_data_size=4096] | o- ana_groups ..................................................................... [...] | | o- 1 ................................................................ [state=optimized] | o- referrals ...................................................................... [...] | o- subsystems ..................................................................... [...] | o- testof .................................................................. [...] o- subsystems ......................................................................... [...] o- testof .................... [version=1.3, allow_any=1, serial=2cb2b6bc4b46653b7680] o- allowed_hosts .................................................................. [...] o- namespaces ..................................................................... [...] o- 1 [path=/dev/zvol/ttt/config/nvme-zvol, uuid=d88dc614-c22d-4942-8a97-5108f210aa1f, grpid=1, enabled] 看一下所有的配置，其实直接按照这个配置去配就行了。 我们最后需要保存一下配置为nvmet-config-loaded.json ，记住一定是这个，因为脚本会自动加载这个命名的配置，并且放在脚本的同目录 1saveconfig nvmet-config-loaded.json 然后加载配置，此时就创建完成了 1/mnt/CPool/myscripts/nvmet-venv/bin/python /root/.local/bin/nvmetcli restore nvmet-config-loaded.json 加入自启动 一定要把下面命令加入开机启动，这样就能自动安装环境并且加载配置 1bash /mnt/CPool/myscripts/scale-nvmet-start.sh 测试机器 这里就简述了，根据你自己环境 配置环境 12345apt update &amp;&amp; apt -y install nvme-cli #tcp modprobe nvme_tcp &amp;&amp; echo &quot;nvme_tcp&quot; &gt;&gt; /etc/modules-load.d/nvme_tcp.conf #rmda modprobe nvme_rdma &amp;&amp; echo &quot;nvme_rdma&quot; &gt;&gt; /etc/modules-load.d/nvme_tcp.conf 查找 1nvme discover -t rdma -a 192.168.2.140 -s 4420 链接 1nvme connect -t rdma -n 刚才那个爱叫什么叫什么的 -a 192.168.2.140 -s 4420 查看 12345nvme list #能看到一个model是linux的硬盘，就是nvmeof挂载的了 Node Generic SN Model Namespace Usage Format FW Rev --------------------- --------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- -------- /dev/nvme4n1 /dev/ng4n1 9c5bb3bf618c2999ef73 Linux 1 53.69 GB / 53.69 GB 512 B + 0 B 6.6.44-p 断开链接 1nvme disconnect -n 刚才那个爱叫什么叫什么的 参考： https://www.intel.com/content/dam/support/us/en/documents/network-and-i-o/fabric-products/Config_NVMe_on_Intel_OPA_AN_J78967_v3_0.pdf https://www.reddit.com/r/truenas/comments/1fh3rfl/an_idiots_walkthrough_to_setting_up_nvmeofroce/ https://linbit.com/blog/configuring-highly-available-nvme-of-attached-storage-in-proxmox-ve/ https://kb.blockbridge.com/technote/proxmox-iscsi-vs-nvmetcp/#","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"NFS over RDMA  TrueNAS","slug":"TrueNAS/NFS over RDMA  TrueNAS","date":"2025-01-01T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"TrueNAS/NFS over RDMA  TrueNAS/","permalink":"https://homelabproject.cc/TrueNAS/NFS%20over%20RDMA%20%20TrueNAS/","excerpt":"","text":"关键 rpcrdma模块加载 /proc/fs/nfsd/portlist写入rdma端口 你可以直接输入下面命令下载脚本并执行。 1curl -o ./load-nfs-rdma.sh https://file.homelabproject.cc/d/local/Server/Src/rdma-tools/load-nfs-rdma.sh 或者自行粘贴 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/bin/bash LOGFILE=&quot;/var/log/rdma_setup.log&quot; RDMA_PORT=${RDMA_PORT:-20049} # Redirect output to log file exec &gt; &gt;(tee -a &quot;$LOGFILE&quot;) 2&gt;&amp;1 echo &quot;Starting RDMA setup script at $(date)&quot; # Ensure script is run as root if [ &quot;$(id -u)&quot; -ne 0 ]; then echo &quot;This script must be run as root. Please re-run with sudo or as root user.&quot; &gt;&amp;2 exit 1 fi # Check if the rpcrdma module is available if ! modinfo rpcrdma &amp;&gt;/dev/null; then echo &quot;The rpcrdma module is not available on this system. Please ensure it is installed.&quot; &gt;&amp;2 exit 1 fi # Check if the rpcrdma module is loaded if ! lsmod | grep -q &quot;^rpcrdma&quot;; then echo &quot;The rpcrdma module is not loaded, loading it now...&quot; modprobe rpcrdma if [ $? -eq 0 ]; then echo &quot;The rpcrdma module was successfully loaded.&quot; else echo &quot;Failed to load the rpcrdma module. Please check permissions or module availability.&quot; &gt;&amp;2 exit 1 fi else echo &quot;The rpcrdma module is already loaded.&quot; fi # Check if /proc/fs/nfsd/portlist contains the RDMA port if ! grep -q &quot;rdma $RDMA_PORT&quot; /proc/fs/nfsd/portlist; then echo &quot;&#39;rdma $RDMA_PORT&#39; is not present in /proc/fs/nfsd/portlist, adding it now...&quot; echo &quot;rdma $RDMA_PORT&quot; &gt; /proc/fs/nfsd/portlist if [ $? -eq 0 ]; then echo &quot;&#39;rdma $RDMA_PORT&#39; was successfully added to /proc/fs/nfsd/portlist.&quot; else echo &quot;Failed to write to /proc/fs/nfsd/portlist. Please check permissions.&quot; &gt;&amp;2 exit 1 fi else echo &quot;&#39;rdma $RDMA_PORT&#39; is already present in /proc/fs/nfsd/portlist.&quot; fi #echo &quot;RDMA setup script completed successfully at $(date)&quot; 另外可以放入定时任务，自动检测模块加载和端口，确保服务正常","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"SMB 10G 传输文件排难指南","slug":"Others/SMB 10G 传输文件排难指南","date":"2024-10-09T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"Others/SMB 10G 传输文件排难指南/","permalink":"https://homelabproject.cc/Others/SMB%2010G%20%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E6%8E%92%E9%9A%BE%E6%8C%87%E5%8D%97/","excerpt":"","text":"入入入入入入入门教程系列？ 前情提要 很多人在组完NAS，尤其是10G往上局域网或者直连的用户，经常会遇到SMB拷贝一个文件，达不到预计速度，然后问：“我这个NAS系统（万兆网卡）怎么跑不满10G？” ：“？“ 这篇文章会记录一些最基础的排难方式。同时这也能学会如何iperf3和fio命令，以及查看PCIe设备速率。 原因&amp;&amp;分析 当你从挂载的SMB共享拷贝到NAS或者本地，其实中间有很多个环节。 以NAS通过10Gbps网卡直连PC，从本地写入一个文件到NAS为例，能跑到10Gbps需要大致以下的条件： Windows本地磁盘的读取速度大于等于10Gbps Windows本地网卡（上行）速度大于等于10Gbps NAS磁盘写入大于等于10Gbps NAS网卡（下行）大于等于10Gbps Windows以及NAS满足SMB 10Gbps单核性能 其实可以看到，大体就是磁盘速度+网速，但是影响这2个因素的问题又有好多。 还有一种架构，即服务器和PC均在10G交换机下，分别通过10G链接到交换机，虽然多了交换机，但是10G交换机基本不会对环境造成比较大的影响，除了记得在开启PC和NAS 巨帧的同时，交换机也要打开就行。 测试方法 Windows本地磁盘的读取速度大于等于10Gbps？ 测试 https://crystalmark.info/en/software/crystaldiskmark/ 请！这还要教？不行就用各种工具箱里面的。 排难 顺序读写能大于1000MB/s就超过万兆网卡速率了，大部分上面跑不到1000MB/s无非就是SATA SSD和机械硬盘，所以拷贝测试万兆的时候把目标硬盘或者源硬盘选择为NVMe硬盘即可。 Windows本地网卡（上行）速度大于等于10Gbps &amp;&amp; NAS网卡（下行）大于等于10Gbps？ 测试 首先，要用iperf3工具，iperf3的测试是纯网络测试和硬盘速度无关，所以能确定NAS到Windows的网络吞吐是否达标。 下载&amp;&amp;安装 Windows需要自行下载工具 Windows工具下载： （官网）https://iperf.fr/download/windows/iperf-3.1.3-win64.zip （本站）https://alist.homelabproject.cc/d/local/Server/Tools/iperf-3.1.3-win64.zip NAS端安装iperf3 Ubuntu/Debian(PVE/OMV) TrueNAS SCALE/CORE以及 UNRAID自带无需安装。 1apt update &amp;&amp; apt install iperf3 -y CentOS(还有人用？会用得也不用教吧（ 1yum install iperf3-y 群晖等其他没用过给不了教程，这边建议”百度“。 使用 iperf3命令分为服务端和客户端，但是在这里的测试中，在哪边跑都行。但是需要先运行服务端。 服务端 1iperf3 -s -p 52239 解释： -s：代表进入服务端模式 -p：表示指定iperf3服务端口，即使不加会有个默认端口，但是为了避免端口冲突我这里制定了一个，实际可以根据你自己随便给个数字都行 客户端 1iperf -c 192.168.0.123 -p 52239 -P 5 -t 10 -d 解释： -c：代表进入客户端模式，需要链接服务端，后面的参数192.168.0.123，替换成刚刚运行服务端的IP地址即可 -p：指定服务端的端口，需要与上方服务端端口一致即可 -P：代表线程数，即到服务端并行的连接数，这边建议照抄给5就行，一般来说能解决单线程跑不满问题，5个线程跑不满10G基本给多了也没用（注意这里的-P是大写） -t：代表运行时间（单位是秒），10以上基本结果差距不大了，可以自行调整或者抄作业 -d：代表测试双向，发送和接受 注意这些命令在NAS系统(Linux)可以直接执行 在Windows下需要先解压刚刚下载的压缩包，然后Win键+R输入cmd打开命令行 输入cd /盘符 &quot;iperf3解压出来文件夹的路径&quot;，路径可以直接复制windows资源管理器上方的地址 例如： 1cd &quot;C:\\Users\\45251\\Downloads\\iperf-3.1.3-win64&quot; 然后由上文的命令开头 ”iperf3“ 变为 ”.\\iperf3.exe“，后面部分一致 效果如图： 分析结果 执行完命令就能看到最后的结果，实例如下： ① Interval表示时间间隔。 ② Transfer表示时间间隔里面转输的数据量。 ③ Bandwidth是时间间隔里的传输速率。 蓝色框数据是最终的结果，也是5组数据（-P 5 参数）的和，而且因为我们设置了-d，有2组数据分别是发送（sender）和接收(receiver)。除此之外有10组数据，因为我们设置了5个线程+同时测试双向。 可以看到在蓝色框里面我们以及跑满了10Gbps的带宽，证明在网络带宽方面，Windows到NAS端没用问题 排难 错误 如果结果如下 Transfer只有7.53GBytes以及Bandwidth6.47Gbits/sec时，即代表带宽出现了问题。那么就需要进行排难了。 （使用了多线程测速，那么往往是否开启巨帧对结果不会有影响） 从经验来看，iperf3测试无法跑满大概率是因为两边的网卡其中一端的PCIe速率无法满足万兆的需求。因为很多用户使用消费级主板都会把万兆网卡插在非第一槽，而很多厂商有喜欢把他们的来自芯片组的PCIeX1 X4带宽的插槽做成X16的物理插槽来”提升扩展性“，实际只能跑在X1或者X4。但是其实PCIe2.0x4或者PCIe3.0x2以上都已经能满足万兆单口的需求了，大部分仔细查看主板说明书，基本能知道。 关于PCIe速率能跑什么速度，可以参考下面的图，记得乘以0.7左右的损耗，比如PCIe2.0X2写的1.00GB/s，好像理论能跑万兆，实际只能跑在6、700MB/s左右的速度 即使主板写的很清楚，但是也不排除一些阴间情况，比如使用一些延长线，掉PCIe速率，甚至是长度。比如我买到过一根标注M2转PCIeX4的线，只能最高跑在X2（？），导致上2.0卡跑在PCIe2.0X2时就无法跑满万兆带宽。 所以最方便的方法是在系统内查看当前协商速度。 Windows：在“设备管理器”中找到你的网卡，双击点开后，点击详细信息，下来属性框，应该能找到2个参数，如下2个参数 PCIe当前链路宽度代表PCIe槽的长度即PCIeX(16/8/4/1) PCIe当前链路速度代表PCIe(4/3/2).0 例如： 即当前设备运行在PCIe1.0 X4 （需要注意的是这里的数值是16进制，不过网卡都是X4 X8的，就不用管就行） 另外如果是Mellanox的卡，是可以在属性的information里面直接看到速率的 Linux： 先查看一下网卡，找到你自己的网卡，可以看到我这里的是c2:00.0 1sudo lspci | grep -i eth 所以再用下面命令查询c2:00.0的vendor id和device id 1lspci -n | grep -i c2:00.0 最后用这个命令就能查询到当前的的链接速度 1lspci -n -d 8086:10fb -vvv | grep -i width 可以看到我的链接速度(Speed)是5GT/s, 宽度（Width）是X8，链接速度可以直接对应上面的表，就可以直接现在这张卡跑在PCIe2.0x8，带宽是没问题的 查询的步骤就分为这2部分，如果有一边低于PCIe2.0x4或者PCIe3.0x2的话，就代表无法跑满万兆，需要更换一个PCIe位置。 NAS磁盘写入大于等于10Gbps？ 测试 这里要用到Fio 是一个功能强大的 I/O 压力测试工具，可以对硬盘进行顺序读写测试。 安装 Ubuntu/Debian(PVE/OMV) 1apt update &amp;&amp; apt install fio -y CentOS(还有人用？会用得也不用教吧（ 1yum install iperf3-y 使用 先用cd 命令进入你要测试的文件系统或者阵列 1cd [需要测试的路径] 然后使用下面命令创建一个FIO测试文件的配置文件 1234567891011cat &gt; test_disk_read.fio &lt;&lt;EOF [seq-read] name=Sequential Read Test ioengine=libaio iodepth=8 rw=read bs=1M size=10G numjobs=16 runtime=60 EOF 1234567891011cat &gt; test_disk_write.fio &lt;&lt;EOF [seq-write] name=Sequential Write Test ioengine=libaio iodepth=8 rw=write bs=1M size=10G numjobs=16 runtime=60 EOF （照抄作业就行，关于具体参数，日后更新吧。。） 开始测试 测试读取 1fio test_disk_read.fio 测试写入 1fio test_disk_write.fio （测试需要一定时间） 最后应该我们会得到这样一个测试结果，READ代表读取，WRITE代表写入，如果2者均超过1000MiB/s，那么在万兆传输这一个问题应该不会产生瓶颈。 123Run status group 0 (all jobs): READ: bw=1072MiB/s (1124MB/s), 1072MiB/s-1072MiB/s (1124MB/s-1124MB/s), io=10.0GiB (10.7GB), run=9553-9553msec WRITE: bw=2003MiB/s (2100MB/s), 2003MiB/s-2003MiB/s (2100MB/s-2100MB/s), io=10.0GiB (10.7GB), run=5113-5113msec 排难 其实这一部分无非也就是你机械阵列的问题，需要优化你的阵列速度，那不同文件系统就相差甚远，需要自己进行调整，这里只能帮忙定位问题。 Windows以及NAS满足SMB 10Gbps单核性能？ 其实这也是非常简单的，Windows传文件的时候打开任务管理，查看在跑不满的时候，是否有一个核心跑满了，那就证明单核性能瓶颈了。（2023年应该不会有U单核这样瓶颈吧） 总结 这篇文件给出了一个最基础问题的定位方式，以及命令简单的理解，基本能解决97%（？）的问题，后面可以考虑再更新一下FIO命令的详细解释，不过这东西网上一堆，这里给出教程只是希望你在着急使用时能快速学会，不会因为看一遍文章，要去翻好几篇才能看懂。 引用&amp;&amp;备注 https://en.wikipedia.org/wiki/PCI_Express “Icon made by Nikita Golubev from www.flaticon.com” “Icon made by Pixel perfect from www.flaticon.com”","categories":[],"tags":[{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"}]},{"title":"Proxmox VE (PVE) CT LXC备份失败","slug":"Proxmox/Proxmox VE (PVE) CT LXC备份失败","date":"2024-04-10T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"Proxmox/Proxmox VE (PVE) CT LXC备份失败/","permalink":"https://homelabproject.cc/Proxmox/Proxmox%20VE%20(PVE)%20CT%20LXC%E5%A4%87%E4%BB%BD%E5%A4%B1%E8%B4%A5/","excerpt":"","text":"问题 在PVE中使用NFS作为存储经常会遇到权限问题，我这里使用FakeNAS~~(TrueNAS SCALE)~~ NFS的共享作为虚拟机的备份，但是会发现非特权容器都会出现备份失败的情况具体情况如下 123456789101112131415161718192021222324252627INFO: starting new backup job: vzdump 112 --notes-template &#39;{{guestname}}&#39; --storage vm_omv --remove 0 --mode snapshot --compress zstd --node serverhub INFO: Starting Backup of VM 112 (lxc) INFO: Backup started at 2023-06-25 14:50:12 INFO: status = running INFO: CT Name: Nginx INFO: including mount point rootfs (&#39;/&#39;) in backup INFO: mode failure - some volumes do not support snapshots INFO: trying &#39;suspend&#39; mode instead INFO: backup mode: suspend INFO: ionice priority: 7 INFO: CT Name: Nginx INFO: including mount point rootfs (&#39;/&#39;) in backup INFO: temporary directory is on NFS, disabling xattr and acl support, consider configuring a local tmpdir via /etc/vzdump.conf INFO: starting first sync /proc/2982556/root/ to /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp INFO: first sync finished - transferred 1.29G bytes in 61s INFO: suspending guest INFO: starting final sync /proc/2982556/root/ to /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp INFO: final sync finished - transferred 1.05M bytes in 1s INFO: resuming guest INFO: guest is online again after 1 seconds INFO: creating vzdump archive &#39;/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tar.zst&#39; INFO: tar: /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp: Cannot open: Permission denied INFO: tar: Error is not recoverable: exiting now ERROR: Backup of VM 112 failed - command &#39;set -o pipefail &amp;&amp; lxc-usernsexec -m u:0:100000:65536 -m g:0:100000:65536 -- tar cpf - --totals --one-file-system -p --sparse --numeric-owner --acls --xattrs &#39;--xattrs-include=user.*&#39; &#39;--xattrs-include=security.capability&#39; &#39;--warning=no-file-ignored&#39; &#39;--warning=no-xattr-write&#39; --one-file-system &#39;--warning=no-file-ignored&#39; &#39;--directory=/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp&#39; ./etc/vzdump/pct.conf ./etc/vzdump/pct.fw &#39;--directory=/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp&#39; --no-anchored &#39;--exclude=lost+found&#39; --anchored &#39;--exclude=./tmp/?*&#39; &#39;--exclude=./var/tmp/?*&#39; &#39;--exclude=./var/run/?*.pid&#39; . | zstd --rsyncable &#39;--threads=1&#39; &gt;/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tar.dat&#39; failed: exit code 2 INFO: Failed at 2023-06-25 14:51:25 INFO: Backup job finished with errors TASK ERROR: job errors 可以看到报错中提示存储NFS 提示权限限制 原因是因为LXC使用的是非root用户，而我在TrueNAS使用Maproot User会仅限于客户端（PVE）的root用户映射到NFS服务端（TrueNAS）的root用户的权限，导致权限出错。 解决方法 更改备份的临时目录到系统盘就能轻松秒杀，但是我的系统盘是16G傲腾，无法支撑临时备份，所以不行。 1echo &quot;tmpdir: /tmp&quot; &gt;&gt; /etc/vzdump.conf 将Maproot的选项更改到Mapall，即 将“NFS服务器将在客户端访问共享时将所有用户的权限映射为root用户的权限，但仅限于root用户”更改为“NFS服务器将在客户端访问共享时将所有用户的权限映射为root用户的权限。”。让客户端（PVE）非root权限(非特权)的LXC容器也能映射为NFS服务端（TrueNAS）root用户。","categories":[],"tags":[{"name":"ProxmoxVE","slug":"ProxmoxVE","permalink":"https://homelabproject.cc/tags/ProxmoxVE/"}]},{"title":"TrueNAS SCALE（Network UPS Tools） 使用CyberPower UPS 10-20小时 断连","slug":"TrueNAS/TrueNAS CyberPower NUT (Network UPS Tools) 断连","date":"2023-05-21T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"TrueNAS/TrueNAS CyberPower NUT (Network UPS Tools) 断连/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20CyberPower%20NUT%20(Network%20UPS%20Tools)%20%E6%96%AD%E8%BF%9E/","excerpt":"","text":"在/usr/nut/ups.conf 或者WebUI-系统设置-服务-UPS-Auxiliary Parameters (ups.conf)中加入 1pollonly = &quot;enabled&quot; 保存，重启ups服务即可 图示： 参考来源：https://github.com/networkupstools/nut/issues/1029","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"文章归档","slug":"Others/Posts","date":"1999-12-31T15:00:00.000Z","updated":"2025-05-29T14:39:51.410Z","comments":true,"path":"/posts/","permalink":"https://homelabproject.cc/posts/","excerpt":"","text":"","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"},{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"},{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"},{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"},{"name":"Incus","slug":"Incus","permalink":"https://homelabproject.cc/tags/Incus/"},{"name":"Hardware","slug":"Hardware","permalink":"https://homelabproject.cc/tags/Hardware/"},{"name":"RouterOS","slug":"RouterOS","permalink":"https://homelabproject.cc/tags/RouterOS/"},{"name":"ProxmoxVE","slug":"ProxmoxVE","permalink":"https://homelabproject.cc/tags/ProxmoxVE/"}]}