{"meta":{"title":"HomeLabProject","subtitle":"","description":"家里开IDC，但是什么都不会，一问三不知","author":"Channing He","url":"https://homelabproject.cc","root":"/"},"pages":[{"title":"","date":"2025-06-19T04:44:25.212Z","updated":"2025-06-19T04:44:25.212Z","comments":true,"path":"custom.css","permalink":"https://homelabproject.cc/custom.css","excerpt":"","text":":root{--gutter:25px;--radius:13px;--color-primary:#1c6c36;--color2:#ff761e;--color3:#ffb900;--color4:#33d57a;--color5:#00dbff;--color6:#1a98ff;--color7:#9090ff;--color-primary-bg:rgba(20, 103, 10, 0.15);--color2-bg:rgba(255,118,30,0.15);--color3-bg:rgba(255,185,0,0.15);--color4-bg:rgba(51,213,122,0.15);--color5-bg:rgba(0,219,255,0.15);--color6-bg:rgba(26,152,255,0.15);--color7-bg:rgba(144,144,255,0.15);--color-shadow:rgba(161, 177, 204, 0.4)}.nexmoe-post-footer{background-color:rgba(16,16,16,0)}.nexmoe-post-copyright{background-color:#ffffff78}article ul li{line-height:1.5rem!important}[data-theme=light]{--hl-color:#90a4ae!important;--hl-bg:#f6f8fa!important;--hltools-bg:#e6ebf1!important;--hltools-color:#90a4ae!important;--hlnumber-bg:#f6f8fa!important;--hlnumber-color:rgba(144, 164, 174, 0.5)!important;--hlscrollbar-bg:#dce4eb!important;--hlexpand-bg:linear-gradient(180deg, rgba(246, 248, 250, 0.1), rgba(246, 248, 250, 0.9))!important}[data-theme=dark]{--hl-color:#abb2bf!important;--hl-bg:#282c34!important;--hltools-bg:#21252b!important;--hltools-color:#bbbbbc!important;--hlnumber-bg:#282c34!important;--hlnumber-color:#495162!important;--hlscrollbar-bg:#373c47!important;--hlexpand-bg:linear-gradient(180deg, rgba(40, 44, 52, 0.1), rgba(40, 44, 52, 0.9))!important}figure.shiki{background-color:var(--hl-bg)!important;color:var(--hl-color)!important}"},{"title":"APT源转Docker环境变量转换工具","date":"2024-06-22T15:00:00.000Z","updated":"2025-06-19T04:44:25.212Z","comments":true,"path":"tools/apt-converter/index.html","permalink":"https://homelabproject.cc/tools/apt-converter/index.html","excerpt":"","text":"APT源转Docker环境变量转换工具 工具介绍 该工具用于将Debian/Ubuntu系统的APT源地址转换为Docker镜像源部署时所需的环境变量格式。 使用说明 在下方输入框中粘贴您的APT源地址（每行一个deb条目） 点击\"转换\"按钮，即可在下方获得转换后的环境变量 点击\"复制结果\"按钮复制转换结果 支持的格式 标准APT源：deb https://example.com/debian bookworm main 带架构的APT源：deb [arch=arm64,amd64] https://example.com/debian bookworm main 带目录的APT源：deb https://example.com/debian bookworm main/debian-installer 多组件APT源：deb https://example.com/debian bookworm main contrib non-free 输入APT源 转换 加载示例 清空 转换结果 复制结果 function convertSources() { const input = document.getElementById('apt-input').value.trim(); if (!input) { alert('请输入APT源'); return; } const result = parseAptSources(input); document.getElementById('apt-output').value = result; } function loadExample() { document.getElementById('apt-input').value = `# Proxmox混合源示例 deb [ arch=amd64,arm64,loong64 ] https://download.lierfang.com/pxcloud/pxvirt/ bookworm main deb [ arch=amd64,arm64 ] https://download.lierfang.com/pxcloud/pxvirt/ bookworm ceph-squid deb [ arch=amd64,arm64 ] https://download.lierfang.com/pxcloud/pxvirt/ bookworm ceph-reef deb [ arch=amd64 ] https://download.lierfang.com/pxcloud/pxvirt/ bookworm ceph-quincy # Tailscale示例 deb [arch=arm64,amd64] https://pkgs.tailscale.com/stable/debian bullseye main deb [arch=arm64,amd64] https://pkgs.tailscale.com/stable/debian bookworm main`; } function clearAll() { document.getElementById('apt-input').value = ''; document.getElementById('apt-output').value = ''; } function copyResult() { const output = document.getElementById('apt-output'); if (!output.value) { alert('没有可复制的内容'); return; } output.select(); if (navigator.clipboard) { navigator.clipboard.writeText(output.value).then(() => { alert('已复制到剪贴板'); }).catch(() => { document.execCommand('copy'); alert('已复制到剪贴板'); }); } else { document.execCommand('copy'); alert('已复制到剪贴板'); } } function parseAptSources(input) { const lines = input.split('\\n') .map(line => line.trim()) .filter(line => line && line.startsWith('deb') && !line.startsWith('#')); if (lines.length === 0) { return '未找到有效的APT源行'; } // 按URL分组 const urlGroups = {}; lines.forEach(line => { try { const parsed = parseLine(line); if (!parsed) return; const { url, dist, components, arch } = parsed; if (!urlGroups[url]) { urlGroups[url] = []; } urlGroups[url].push({ dist, components, arch }); } catch (error) { console.warn('解析失败:', line, error); } }); // 生成环境变量 let result = ''; Object.keys(urlGroups).forEach(url => { result += `APTSYNC_URL=${url}\\n\\n`; const dists = urlGroups[url]; let distsStr = 'APTSYNC_DISTS='; dists.forEach((dist, index) => { if (index > 0) distsStr += ':'; // 格式: release|component|arch|download_dir distsStr += `${dist.dist}|${dist.components}|${dist.arch}|`; }); result += distsStr + '\\n\\n'; }); return result.trim(); } function parseLine(line) { // 先处理架构选项，然后再按空格分割 let cleanLine = line.trim(); let arch = 'amd64'; // 检查是否有架构选项 const archMatch = cleanLine.match(/\\[\\s*arch\\s*=\\s*([^\\]]+)\\s*\\]/); if (archMatch) { arch = archMatch[1].split(',').map(a => a.trim()).join(' '); // 移除架构选项部分 cleanLine = cleanLine.replace(/\\[\\s*arch\\s*=\\s*[^\\]]+\\s*\\]/, '').replace(/\\s+/g, ' ').trim(); } const parts = cleanLine.split(' '); if (parts[0] !== 'deb' || parts.length < 4) { return null; } const url = parts[1].replace(/\\/$/, ''); const dist = parts[2]; const components = parts.slice(3).join(' ').split('/')[0]; return { url, dist, components, arch }; }"},{"title":"","date":"2025-06-19T04:44:26.925Z","updated":"2025-06-19T04:44:26.925Z","comments":true,"path":"images/others/logdy.json","permalink":"https://homelabproject.cc/images/others/logdy.json","excerpt":"","text":"{\"name\":\"main\",\"columns\":[{\"id\":\"196505\",\"name\":\"StartLocal\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['StartLocal'] }\\n}\",\"idx\":27,\"width\":152},{\"id\":\"901886\",\"name\":\"RequestHost\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestHost'] }\\n}\",\"idx\":15,\"width\":144},{\"id\":\"299677\",\"name\":\"column request_X-Forwarded-Proto\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['request_X-Forwarded-Proto'] }\\n}\",\"idx\":73,\"width\":150},{\"id\":\"506966\",\"name\":\"ClientHost\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['ClientHost'] }\\n}\",\"idx\":2,\"width\":310,\"faceted\":true},{\"id\":\"807219\",\"name\":\"ClientAddr\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['ClientAddr'] }\\n}\",\"idx\":1,\"width\":150,\"hidden\":true},{\"id\":\"613622\",\"name\":\"ClientPort\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['ClientPort'] }\\n}\",\"idx\":3,\"width\":150,\"hidden\":true},{\"id\":\"533527\",\"name\":\"Verb\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestMethod'] }\\n}\",\"idx\":16,\"width\":61,\"hidden\":true},{\"id\":\"257448\",\"name\":\"RequestPath\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestPath'] }\\n}\",\"idx\":17,\"width\":283},{\"id\":\"673790\",\"name\":\"Proto\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestProtocol'] }\\n}\",\"idx\":19,\"width\":67},{\"id\":\"127578\",\"name\":\"Scheme\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestScheme'] }\\n}\",\"idx\":20,\"width\":60,\"hidden\":true},{\"id\":\"415466\",\"name\":\"RSize\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestContentSize'] }\\n}\",\"idx\":13,\"width\":66,\"hidden\":true},{\"id\":\"196253\",\"name\":\"DStatus\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['DownstreamStatus'] }\\n}\",\"idx\":6,\"width\":71,\"hidden\":true},{\"id\":\"227251\",\"name\":\"DSize\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['DownstreamContentSize'] }\\n}\",\"idx\":5,\"width\":84,\"hidden\":true},{\"id\":\"593531\",\"name\":\"uaURL\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['uaUrl'] }\\n}\",\"faceted\":false,\"idx\":38,\"width\":150,\"hidden\":true},{\"id\":\"536519\",\"name\":\"User Agent\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['request_User-Agent'] }\\n}\",\"idx\":33,\"width\":335},{\"id\":\"166747\",\"name\":\"Duration\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['Duration'] }\\n}\",\"idx\":7,\"width\":95},{\"id\":\"756429\",\"name\":\"Overhead\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['Overhead'] }\\n}\",\"idx\":11,\"width\":95},{\"id\":\"205334\",\"name\":\"raw\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.content || \\\"-\\\"}\\n }\",\"idx\":0,\"width\":150,\"hidden\":true},{\"id\":\"796875\",\"name\":\"ServiceName\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['ServiceName'] }\\n}\",\"idx\":24,\"width\":115},{\"id\":\"360113\",\"name\":\"OriginContentSize\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['OriginContentSize'] }\\n}\",\"idx\":8,\"width\":150,\"hidden\":true},{\"id\":\"446550\",\"name\":\"OriginDuration\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['OriginDuration'] }\\n}\",\"idx\":9,\"width\":150,\"hidden\":true},{\"id\":\"210488\",\"name\":\"OriginStatus\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['OriginStatus'] }\\n}\",\"idx\":10,\"width\":150,\"hidden\":true},{\"id\":\"654768\",\"name\":\"RequestAddr\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestAddr'] }\\n}\",\"idx\":12,\"width\":150,\"hidden\":true},{\"id\":\"982865\",\"name\":\"RequestCount\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestCount'] }\\n}\",\"idx\":14,\"width\":150,\"hidden\":true},{\"id\":\"146433\",\"name\":\"RequestPort\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RequestPort'] }\\n}\",\"idx\":18,\"width\":150,\"hidden\":true},{\"id\":\"763017\",\"name\":\"RetryAttempts\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RetryAttempts'] }\\n}\",\"idx\":21,\"width\":150,\"hidden\":true},{\"id\":\"498987\",\"name\":\"RouterName\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['RouterName'] }\\n}\",\"idx\":22,\"width\":130},{\"id\":\"671214\",\"name\":\"ServiceAddr\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['ServiceAddr'] }\\n}\",\"idx\":23,\"width\":150,\"hidden\":true},{\"id\":\"909536\",\"name\":\"ServiceURL\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['ServiceURL'] }\\n}\",\"idx\":25,\"width\":150,\"hidden\":true},{\"id\":\"857001\",\"name\":\"SpanId\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['SpanId'] }\\n}\",\"idx\":26,\"width\":150,\"hidden\":true},{\"id\":\"872847\",\"name\":\"StartUTC\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['StartUTC'] }\\n}\",\"idx\":28,\"width\":150,\"hidden\":true},{\"id\":\"275331\",\"name\":\"TraceId\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['TraceId'] }\\n}\",\"idx\":29,\"width\":150,\"hidden\":true},{\"id\":\"933666\",\"name\":\"entryPointName\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['entryPointName'] }\\n}\",\"idx\":30,\"width\":150},{\"id\":\"349392\",\"name\":\"level\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['level'] }\\n}\",\"idx\":31,\"width\":150,\"hidden\":true},{\"id\":\"138481\",\"name\":\"msg\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['msg'] }\\n}\",\"idx\":32,\"width\":150,\"hidden\":true},{\"id\":\"562160\",\"name\":\"time\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['time'] }\\n}\",\"idx\":34,\"width\":150,\"hidden\":true},{\"id\":\"483561\",\"name\":\"hostname\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['hostname'] }\\n}\",\"faceted\":true,\"idx\":34,\"width\":150,\"hidden\":true},{\"id\":\"090942\",\"name\":\"subdomain\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['subdomain'] }\\n}\",\"faceted\":true,\"idx\":35,\"width\":150,\"hidden\":true},{\"id\":\"837141\",\"name\":\"external\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['external'] }\\n}\",\"faceted\":true,\"idx\":36,\"width\":150,\"hidden\":true},{\"id\":\"612490\",\"name\":\"me\",\"handlerTsCode\":\"(line: Message): CellHandler => {\\n return { text: line.json_content['me'] }\\n}\",\"faceted\":true,\"idx\":37,\"width\":150,\"hidden\":true}],\"settings\":{\"leftColWidth\":300,\"drawerColWidth\":900,\"maxMessages\":1000,\"middlewares\":[{\"id\":\"m_632328\",\"name\":\"additional_info\",\"handlerTsCode\":\"(line: Message): Message | void => {\\n\\n let external = true;\\n let me = false;\\n\\n const client = line.json_content.ClientHost;\\n const host = line.json_content.RequestHost;\\n const uaString = line.json_content['request_User-Agent'];\\n\\n external = !client.includes('192.168.0') && client !== \\\"2600:1700:1e1f:YOUR:IPV6\\\";\\n me = client === \\\"192.168.TREFIK_EXTERNAL.HOST_CHANGEME\\\" || client === \\\"2600:1700:1e1f:YOUR:IPV6CHANGME\\\"\\n\\n const urlPattern = /^(?:([A-Za-z]+?):\\\\/\\\\/)?(?:[^@\\\\n]+@)?(?:([A-Za-z]+?)\\\\.)?([^:\\\\/\\\\n?]+)(.*)/;\\n\\n let hostname = '-',\\n subdomain = '-',\\n uaUrl;\\n\\n if (host !== '-') {\\n\\n const urlMatch = host.match(urlPattern);\\n\\n if (urlMatch) {\\n if (urlMatch[3].includes('.')) {\\n subdomain = urlMatch[2];\\n hostname = urlMatch[3];\\n } else {\\n hostname = `${urlMatch[2]}.${urlMatch[3]}`;\\n }\\n if (external && hostname === \\\"CHANGEME.casa\\\") {\\n external = false;\\n }\\n }\\n }\\n\\n const regUrlPattern = /\\\\+([(http(s)?):\\\\/\\\\/(www\\\\.)?a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,20}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*))/;\\n const uaMatch = uaString.match(regUrlPattern);\\n if (uaMatch) {\\n uaUrl = uaMatch[1];\\n }\\n\\n line.json_content.subdomain = subdomain;\\n line.json_content.hostname = hostname;\\n line.json_content.external = external;\\n line.json_content.me = me;\\n line.json_content.uaUrl = uaUrl;\\n\\n return line;\\n}\"}],\"entriesOrder\":\"desc\"}}"}],"posts":[{"title":"TrueNAS Grafana 监控教程 graphite exporter","slug":"TrueNAS/TrueNAS Grafana 监控教程 graphite exporter","date":"2025-06-18T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/TrueNAS Grafana 监控教程 graphite exporter/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20Grafana%20%E7%9B%91%E6%8E%A7%E6%95%99%E7%A8%8B%20graphite%20exporter/","excerpt":"","text":"来源 https://github.com/Supporterino/truenas-graphite-to-prometheus 还是建议自己看文档 PS：25.04的官方exporter关闭了很多信息，所以采集不全，比如内存和Disk信息。 介绍 首先需要个Grafana监控，主要是Grafana上看一切方便点。 原理 官方用Netdata，早期还直接提供netdata面板，为了&quot;安全&quot;，现在只有exporter，同时是GRAPHITE的，那么就需要个Graphite到Prometheus的转换，来源的链接给你做好了。 部署 Graphite-Prometheus的转换 Docker 没什么要说的，这里我用的39109连接TrueNSA ，39108是给prometheus连接的 12345678services: ts-graphite-prometheus: image: ghcr.io/supporterino/truenas-graphite-to-prometheus ports: #to graphite - 39109:9109 #to prometheus - 39108:9108 TrueNAS exporter设置 如下，要注意的是 Destination IP和Port是你部署上面那个Docker的IP和to graphite的Port。 Perfix注意是truenas不能改 Namespace随意 其他照抄 部署Grafana和Prometheus 网上到处都是，我随便给个例子 首先是部署 12345678910111213141516171819202122232425services: prometheus: image: prom/prometheus container_name: prometheus ports: - 58474:9090 networks: - monitor volumes: - $DOCKER_DATA/prometheus/config:/etc/prometheus command: - --config.file=/etc/prometheus/prometheus.yml restart: always grafana-oss: container_name: grafana restart: always networks: - monitor ports: - 38533:3000 volumes: - $DOCKER_DATA/grafana/config:/var/lib/grafana image: grafana/grafana-oss networks: - monitor 然后你需要去修改Prometheus的配置，你上面创建完没配置也会提示报错 我的路径在$DOCKER_DATA/prometheus/config/prometheus.yml，你们的自己看着替换 最后要加个Truenas的job，targets需要修改成上面部署那个Docker的IP和to Prometheus的Port。 123456789101112global: scrape_interval: 15s # By default, scrape targets every 15 seconds. scrape_configs: - job_name: &#39;prometheus&#39; scrape_interval: 5s static_configs: - targets: [&#39;localhost:9090&#39;] - job_name: &#39;truenas&#39; scrape_interval: 1s static_configs: - targets: [&#39;10.1.10.5:39108&#39;] 重启一下你的Prometheus。 Grafana添加数据源和面板 数据源： Connection - Data sources - Add New data source - 选择 Prometheus - 在Prometheus server URL里填入http://prometheus:9090- 到最后Save $ test 即可 ` 面板： 首先选择你的面板 https://github.com/Supporterino/truenas-graphite-to-prometheus/tree/main/dashboards 正常直接用truenas_scale.json，复制里面内容 Dashboard - New - Import - 在Import via dashboard JSON model里 粘贴 - Load - Mimir里面选你刚刚创建的数据源，就行了。","categories":[],"tags":[{"name":"SelfHosted","slug":"SelfHosted","permalink":"https://homelabproject.cc/tags/SelfHosted/"},{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"Tinyauth 自托管 身份验证 中间件","slug":"Container/Tinyauth 自托管 身份验证 中间件","date":"2025-06-06T15:00:00.000Z","updated":"2025-06-19T04:44:26.917Z","comments":true,"path":"Container/Tinyauth 自托管 身份验证 中间件/","permalink":"https://homelabproject.cc/Container/Tinyauth%20%E8%87%AA%E6%89%98%E7%AE%A1%20%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81%20%E4%B8%AD%E9%97%B4%E4%BB%B6/","excerpt":"","text":"Source &amp;&amp; Doc https://github.com/steveiliop56/tinyauth?tab=readme-ov-file https://tinyauth.app/docs/about.html 强烈建议阅读，此文章只是个推荐，重复造轮子 介绍 首先说一下需求，在Selfhost 缺少身份验证或者是类似PHP等这种很容易存在漏洞应用，需要多一层验证（当然最佳实践应该是直接使用VPN内部访问，不公开到公网，但是有些应用还是存在这种使用场景）。 本身其实有大量的身份验证应用比如Authelia/Authentik/Keycloack之类。 原因1 大部分这些工具为了企业级生产力需求会十分复杂。比如之前使用的Authentik，他的compose需要简简单单147行（ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142services: authentik-postgresql: image: docker.io/library/postgres:16-alpine restart: unless-stopped healthcheck: test: [&quot;CMD-SHELL&quot;, &quot;pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}&quot;] start_period: 20s interval: 30s retries: 5 timeout: 5s volumes: - ./database:/var/lib/postgresql/data environment: POSTGRES_PASSWORD: ${PG_PASS:?database password required} POSTGRES_USER: ${PG_USER:-authentik} POSTGRES_DB: ${PG_DB:-authentik} env_file: - .env networks: - default authentik-redis: image: docker.io/library/redis:alpine command: --save 60 1 --loglevel warning restart: unless-stopped healthcheck: test: [&quot;CMD-SHELL&quot;, &quot;redis-cli ping | grep PONG&quot;] start_period: 20s interval: 30s retries: 5 timeout: 3s volumes: - redis_authentik:/data networks: - default authentik-server: image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG} restart: unless-stopped command: server environment: AUTHENTIK_REDIS__HOST: authentik-redis AUTHENTIK_POSTGRESQL__HOST: authentik-postgresql AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik} AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik} AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS} volumes: - ./authentik/media:/media - ./authentik/custom-templates:/templates # - /var/run/docker.sock:/var/run/docker.sock env_file: - .env # ports: # - &quot;${COMPOSE_PORT_HTTP:-9000}:9000&quot; # - &quot;${COMPOSE_PORT_HTTPS:-9443}:9443&quot; networks: - public_web - default depends_on: authentik-postgresql: condition: service_healthy authentik-redis: condition: service_healthy labels: homepage.group: Networking homepage.name: Authentik homepage.icon: authentik homepage.href: &quot;https://auth.CHANGEME.com&quot; traefik.enable: true traefik.http.routers.authentik.rule: Host(`auth.CHANGEME.com`) traefik.http.services.authentik.loadbalancer.server.port: 9000 traefik.docker.network: public_web traefik.public: true authentik-proxy: image: ghcr.io/goauthentik/proxy:${AUTHENTIK_TAG:-2024.10.5} # ports: # - 9000:9000 # - 9443:9443 networks: - public_web - default environment: AUTHENTIK_REDIS__HOST: authentik-redis AUTHENTIK_HOST: http://authentik-server:9000 AUTHENTIK_INSECURE: &quot;true&quot; AUTHENTIK_TOKEN: ${AUTHENTIK_TRAEFIK_EXT_OUTPOST_TOKEN} # Starting with 2021.9, you can optionally set this too # when authentik_host for internal communication doesn&#39;t match the public URL AUTHENTIK_HOST_BROWSER: https://auth.CHANGEME.com # for logging edit log_level in outpost config from ui labels: traefik.enable: true traefik.port: 9000 traefik.http.routers.authentik-proxy.rule: Host(`CHANGEME.com`) &amp;&amp; PathPrefix(`/outpost.goauthentik.io/`) # `authentik-proxy` refers to the service name in the compose file. traefik.http.middlewares.authentik-proxy.forwardauth.address: http://authentik-proxy:9000/outpost.goauthentik.io/auth/traefik traefik.http.middlewares.authentik-proxy.forwardauth.trustForwardHeader: true traefik.http.middlewares.authentik-proxy.forwardauth.authResponseHeaders: X-authentik-username,X-authentik-groups,X-authentik-entitlements,X-authentik-email,X-authentik-name,X-authentik-uid,X-authentik-jwt,X-authentik-meta-jwks,X-authentik-meta-outpost,X-authentik-meta-provider,X-authentik-meta-app,X-authentik-meta-version traefik.public: true kop.namespace: none restart: unless-stopped worker: image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2024.10.5} restart: unless-stopped command: worker environment: AUTHENTIK_REDIS__HOST: authentik-redis AUTHENTIK_POSTGRESQL__HOST: authentik-postgresql AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik} AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik} AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS} # `user: root` and the docker socket volume are optional. # See more for the docker socket integration here: # https://goauthentik.io/docs/outposts/integrations/docker # Removing `user: root` also prevents the worker from fixing the permissions # on the mounted folders, so when removing this make sure the folders have the correct UID/GID # (1000:1000 by default) user: root volumes: - /var/run/docker.sock:/var/run/docker.sock - ./authentik/media:/media - ./authentik/certs:/certs - ./authentik/custom-templates:/templates env_file: - .env depends_on: authentik-postgresql: condition: service_healthy authentik-redis: condition: service_healthy networks: - default networks: default: internal: true public_web: external: true volumes: redis_authentik: driver: local 原因2 最近把家里的架构由Nginx手动创建变成了Compose + Traefik，现在我创建一个服务的反向代理，只需要向下面这样，在docker compose，反向代理将自动化生成。所以支持通过labels描述，并且自动加载 1234567891011121314151617labels: traefik.enable: true traefik.public: true traefik.docker.network: traefik_external_public_web #----- traefik.http.routers.hlpj-alist.rule: Host(`alist.homelabproject.cc`)||Host(`file.homelabproject.cc`) traefik.http.routers.hlpj-alist.entrypoints: cf traefik.http.services.hlpj-alist.loadbalancer.server.port: 5244 traefik.http.routers.hlpj-alist.service: hlpj-alist #---cloudflared--- cloudflare.tunnel.enable: &quot;true&quot; cloudflare.tunnel.0.hostname: alist.homelabproject.cc cloudflare.tunnel.0.service: http://traefik:808 cloudflare.tunnel.0.zonename: homelabproject.cc cloudflare.tunnel.1.hostname: file.homelabproject.cc cloudflare.tunnel.1.service: http://traefik:808 cloudflare.tunnel.1.zonename: homelabproject.cc 配置 基础容器 首先你需要最基础3个条件 SECERT变量 直接在命令行行随机生成一个32字节的随机密钥 1openssl rand -base64 32 | tr -dc &#39;a-zA-Z0-9&#39; | head -c 32 USERS 你的用户名密码，命令行输入下面，会进入交互，记得在问你是否使用docker的时候选择yes 1docker run -i -t --rm ghcr.io/steveiliop56/tinyauth:v3 user create --interactive 你就会得到转换完用于登陆的账号密码的bcrypt 哈希。如果你有多个账号密码用,隔开 用于验证的域名 tinyauth的访问流程：当你访问需要身份验证的域名，他会转条到tinyauth.你的域名.com验证账号密码，然后你再回到你服务的域名就能正常访问了。 所以需要提前配置好你的域名 最后，配置你下面的环境变量，启动即可 123456789101112tinyauth: image: ghcr.io/steveiliop56/tinyauth:v3 container_name: tinyauth restart: unless-stopped environment: - SECRET=some-random-32-chars-string - APP_URL=https://tinyauth.你的域名.com - USERS=your-username-password-hash labels: traefik.enable: true traefik.http.routers.tinyauth.rule: Host(`tinyauth.你的域名.com`) traefik.http.middlewares.tinyauth.forwardauth.address: http://tinyauth:3000/api/auth/traefik 配置需要验证的应用 你只需要附带Label，添加Router的中间件，即可访问 12labels: traefik.http.routers.[你的服务].middlewares: tinyauth@docker 总结 tinyauth的目的本身就不是为了生产力，对于Homelab来说就是简洁高效，个人觉得还是挺好用。","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"},{"name":"SelfHosted","slug":"SelfHosted","permalink":"https://homelabproject.cc/tags/SelfHosted/"},{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"}]},{"title":"Hexo Blog方案","slug":"Infra/Hexo Blog方案","date":"2025-05-29T15:00:00.000Z","updated":"2025-06-19T04:44:26.917Z","comments":true,"path":"Infra/Hexo Blog方案/","permalink":"https://homelabproject.cc/Infra/Hexo%20Blog%E6%96%B9%E6%A1%88/","excerpt":"","text":"拆分Posts路径到单独Branch，Obsidian挂Posts Branch，本地Markdown只有文章部分，写完直接推送触发编译 本地Gitea Action编译-&gt;只推送静态资源到Github Vercel托管，可以享受较快的国内的访问速度，感谢下面项目提供优质cname解析 https://github.com/xingpingcn/enhanced-FaaS-in-China Gitea Workflow 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374name: Build Hexo Blog on: push: branches: - posts # This workflow will be triggered when pushing to posts branch workflow_dispatch: # Allow manual trigger jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 with: fetch-depth: 0 # Fetch all history for all branches - name: Setup Node.js uses: actions/setup-node@v4 with: node-version: &#39;20&#39; # - name: Clean posts directory # run: | # rm -rf source/* - name: Copy posts run: | # 获取posts分支 git fetch origin posts:posts # 创建临时目录 mkdir -p temp_posts cd temp_posts # 检出posts分支的内容 git --git-dir=../.git --work-tree=. checkout posts -- . # 复制所有文件到source/ cp -r * ../source/ cd .. # 清理临时目录 rm -rf temp_posts - name: Install dependencies run: npm install - name: Build Hexo site run: npx hexo generate - name: Push to GitHub env: HLPJTOKEN: ${{ secrets.HLPJTOKEN }} ACTOR: ${{ vars.GITHUBACTOR }} REPOSITORY: ${{ vars.GITHUBREPOSITORY }} run: | # 克隆目标仓库 git clone &quot;https://${ACTOR}:${HLPJTOKEN}@github.com/${REPOSITORY}.git&quot; temp_deploy cd temp_deploy # 清空仓库内容（保留.git目录） find . -mindepth 1 -maxdepth 1 ! -name &#39;.git&#39; -exec rm -rf {} + # 复制新生成的文件 cp -r ../public/* . # 配置git git config user.name &quot;${ACTOR}&quot; git config user.email &quot;${ACTOR}@users.noreply.github.com&quot; # 添加所有文件并提交 git add -A git commit -m &quot;Build: Update site content&quot; # 推送到main分支 git push origin HEAD:main","categories":[],"tags":[{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"}]},{"title":"自建 APT 镜像源 教程 Tunasync CRON定时 多线程","slug":"Infra/自建 APT 镜像源 教程 Tunasync CRON定时 多线程","date":"2025-05-27T15:00:00.000Z","updated":"2025-06-19T04:44:26.917Z","comments":true,"path":"Infra/自建 APT 镜像源 教程 Tunasync CRON定时 多线程/","permalink":"https://homelabproject.cc/Infra/%E8%87%AA%E5%BB%BA%20APT%20%E9%95%9C%E5%83%8F%E6%BA%90%20%E6%95%99%E7%A8%8B%20Tunasync%20CRON%E5%AE%9A%E6%97%B6%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"源码 Github已经发布，README包含大部分说明 https://github.com/ChanningHe/tunasync-docker 自建原因 家里的服务器越来越多，基础镜像比如Debian之类的需要一个镜像，方便本地快速拉取。 ProxmoxVE的非订阅源用户人数多，还是用起来不是那么快，加上集群机器多。 TrueNAS编译的时候严重依赖他们的APT源，他们的APT源又奇慢，加载只有几百KB/s，换完本地后编译从2h+变成1h+，感知还是明显的。 Tailscale源还是国内访问十分慢的源，用的机器镜像拉不下来，很多校园源又不做商业软件的源。 Compose示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960services: mirrors-nginx: image: nginx:latest container_name: mirrors-nginx ports: - &quot;[Node-Port]:80&quot; volumes: - $DOCKER_DATA/mirrors-nginx/nginx:/etc/nginx/conf.d - $DOCKER_DATA/mirrors-data:/mnt/Kiwi/Mirror/mirrors-data restart: always aptsync-proxmox: environment: - PUID=[CHANGE-UID] - PGID=[CHANGE-GID] - APTSYNC_UNLINK=1 - APTSYNC_URL=http://download.proxmox.com/debian/pve - APTSYNC_DISTS=bookworm|pve-no-subscription|amd64| - CRON=20 23,6,12,20 * * * - TO=/data volumes: - $DOCKER_DATA/mirrors-data/proxmox:/data - $DOCKER_DATA/mirrors-logs/aptsync-proxmox:/log - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro image: channinghe/apt-sync:aria2c aptsync-tailscale: environment: - PUID=[CHANGE-UID] - PGID=[CHANGE-GID] - APTSYNC_UNLINK=1 - APTSYNC_URL=https://pkgs.tailscale.com/stable/debian - APTSYNC_DISTS=bookworm|main|arm64 amd64|:bullseye|main|arm64 amd64| - CRON=20 23,6,12,18 * * * - TO=/data volumes: - $DOCKER_DATA/mirrors-data/tailscale:/data - $DOCKER_DATA/mirrors-logs/tailscale:/log - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro image: channinghe/apt-sync:aria2c aptsync-debian: environment: - PUID=[CHANGE-UID] - PGID=[CHANGE-GID] - APTSYNC_UNLINK=1 - APTSYNC_URL=http://deb.debian.org/debian - APTSYNC_DISTS=bookworm|main contrib non-free non-free-firmware|amd64 arm64|:bullseye|main contrib non-free|amd64 arm64| - CRON=&quot;0 0,6,12,18 * * *&quot; - TO=/data # 下载线程数 - PARALLEL_DOWNLOADS=8 volumes: - $DOCKER_DATA/mirrors-data/debian:/data - $DOCKER_DATA/mirrors-logs/aptsync-debian:/log image: channinghe/apt-sync:aria2c networks: {} Nginx Nginx的目的是公开网页，需要另外增加配置/etc/nginx/conf.d/default.conf(容器内地址) 或者是$DOCKER_DATA/mirrors-nginx/nginx/default.conf(主机地址) 1234567location / { #你挂载进去的apt镜像数据的地方志 alias $DOCKER_DATA/mirrors-data/; autoindex on; autoindex_exact_size off; autoindex_localtime on; } 环境变量 变量名 描述 APTSYNC_URL 上游镜像URL APTSYNC_DISTS 需要同步的发行版配置，格式为：发行版|组件|架构|下载路径 APTSYNC_UNLINK 是否先删除目标文件，设置为1开启 APTSYNC_DISTS，可以使用下面的工具批量转换 🔄 APT源转换工具 将Debian/Ubuntu的APT源地址转换为Docker镜像源部署环境变量 自建源地址 https://mirrors.homelabproject.cc/ （只是做个示例，在Cloudflare CDN前只是公开图一乐） .tools-container { max-width: 800px; margin: 0 auto; padding: 20px; } .tools-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(250px, 1fr)); gap: 20px; margin-top: 30px; } .tool-card { display: block; padding: 20px; border-radius: 8px; background-color: var(--color-card); box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1); transition: transform 0.2s, box-shadow 0.2s; text-decoration: none; color: var(--color-text); } .tool-card:hover { transform: translateY(-5px); box-shadow: 0 5px 15px rgba(0, 0, 0, 0.15); } .tool-icon { font-size: 2.5rem; margin-bottom: 15px; } .tool-card h3 { margin: 0 0 10px 0; font-size: 1.2rem; } .tool-card p { margin: 0; font-size: 0.9rem; opacity: 0.8; } @media (max-width: 600px) { .tools-grid { grid-template-columns: 1fr; } }","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"},{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"},{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"}]},{"title":"TrueNAS 25.04 Docker最佳实践（文件权限篇）","slug":"TrueNAS/TrueNAS 25.04  Docker最佳实践（文件权限UID篇）","date":"2025-05-24T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/TrueNAS 25.04  Docker最佳实践（文件权限UID篇）/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%2025.04%20%20Docker%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90UID%E7%AF%87%EF%BC%89/","excerpt":"","text":"介绍 TrueNAS SCALE 24.10从K8s的支持转到了更好的维护的Docker（中间价需要调用的东西太多了），基本上告别SCALE这个词（这也就是25.04已经不叫SCALE的原因）。 Docker本身对于应用的部署难度是大幅降度的，但是Docker难度的降低其实是有明显代价的，就是权限问题。 这片文章目的在于分析当前Docker镜像带来的各种问题 虽然实在TrueNAS 25.04运行，但是理论在任何系统都通用，都可以作为docker运行的一个 目的 安全第一 因为有对外服务，要保证容器即使被逃逸也不会得到host的root权限 挂载目录权限统一 所有的docker容器应用最后都属于一个UID或者GID，方便权限管理 性能 逃不开的话题，虽然有nfs over rdma能降低挂给其他虚拟机损耗，但是本身zfs对于nvme性能的损耗已经十分大，能通过直接挂载目录而非网络挂载，在CPU，磁盘等方面的消耗也能减少。同时在host管理目录也更加方便。 问题 Docker Daemon 首先众所周知的Docker Daemon（守护进程）永远是root运行。 默认进程和UID Docker 默认的进程或者在容器内的能获得到的root权限就是宿主的root ，因为默认没有启用 user namespace remapping，所以如果容器逃逸了、或者被赋予了访问宿主资源的能力，它就能像宿主机 root 一样行动。 Docker出的解决方法 USER 选项：让容器内的进程直接都以指定UID运行，但是带来很大问题问题是，很多镜像会操作非挂载的系统目录，最简单的比如说/var/log/ 下创建log文件，都会因为非root无法创建，直接无法运行。 Env变量指定UID：这个就是更是阴间中的阴间。因为这个不是一个标准操作，每个镜像作者都有自己想法。 例子1 比如著名的linuxserver/的镜像，他特别贴心的让他们发布的每一个镜像都支持指定PUID和PGID，似乎这是一个能解决，但是你可以看到他们具体的实现方式。 在容器启动的是读取PUID env，非默认的情况下创建对应用户，然后对挂载目录执行lsiown https://github.com/linuxserver/docker-baseimage-ubuntu/blob/212ab2133eb21e4e4b536039c790201baf81fd4f/root/etc/s6-overlay/s6-rc.d/init-adduser/run#L53 123456789101112131415161718192021#... #... PUID=${PUID:-911} PGID=${PGID:-911} if [[ -z ${LSIO_READ_ONLY_FS} ]] &amp;&amp; [[ -z ${LSIO_NON_ROOT_USER} ]]; then USERHOME=$(grep abc /etc/passwd | cut -d &quot;:&quot; -f6) usermod -d &quot;/root&quot; abc groupmod -o -g &quot;${PGID}&quot; abc usermod -o -u &quot;${PUID}&quot; abc usermod -d &quot;${USERHOME}&quot; abc fi #... #... if [[ -z ${LSIO_READ_ONLY_FS} ]] &amp;&amp; [[ -z ${LSIO_NON_ROOT_USER} ]]; then lsiown abc:abc /app lsiown abc:abc /config lsiown abc:abc /defaults fi lsiown好像也是他们自己制作的脚本本质就是chown一下。 https://github.com/linuxserver/docker-mods/blob/cac9e7450a0698f19d750b67db61c4aa214d5290/lsiown.v1#L30 1/usr/bin/find &quot;${PATH[@]}&quot; &quot;${MAXDEPTH[@]}&quot; ! -xtype l \\( ! -group &quot;${GROUP}&quot; -o ! -user &quot;${USER}&quot; \\) -exec chown &quot;${OPTIONS[@]}&quot; &quot;${USER}&quot;:&quot;${GROUP}&quot; {} + || printf &quot;${ERROR}&quot; 只对Dockerfile里面定义的路径chown，对于媒体库这样挂载路径多样的情况，根本照顾不到 用root对挂载进来的“指定配置路径”进行了chown，更改了所有者直接破坏原来的权限。 只有应用进程本身是UID是指定的UID，容器内本身还保留root权限，那么这个容器本身还是存在root用户并且可以调用，只解决文件权限问题，而且是破坏性解决 不是啊，说到底你还是能拿root啊（ 例子2 最近碰到一个更加奇怪（阴间）的镜像 —&gt; Gitea ，他官方提供了2个镜像，一个是basic的一个是rootless 先来看下面的basic，支持USER_UID指定，那么基本来说应该是linuxserver的镜像差不多的方式。 12345678910111213services: server: image: docker.gitea.com/gitea:1.23.8 restart: always environment: - USER_UID=1000 - USER_GID=1000 volumes: - ./data:/var/lib/gitea - ./config:/etc/gitea ports: - &quot;3000:3000&quot; - &quot;2222:2222&quot; 容器内本身还有root权限，通过entrypoint运行脚本，替换passwd文件里面git用户的uid，其实也都属于常规操作。 https://github.com/go-gitea/gitea/blob/688da55f543f82265cc7df2bd1cf2bce53188b7a/docker/root/usr/bin/entrypoint#L22 123456789if [ -n &quot;${USER_GID}&quot; ] &amp;&amp; [ &quot;${USER_GID}&quot; != &quot;`id -g ${USER}`&quot; ]; then sed -i -e &quot;s/^${USER}:\\([^:]*\\):[0-9]*/${USER}:\\1:${USER_GID}/&quot; /etc/group sed -i -e &quot;s/^${USER}:\\([^:]*\\):\\([0-9]*\\):[0-9]*/${USER}:\\1:\\2:${USER_GID}/&quot; /etc/passwd fi # Change UID for USER? if [ -n &quot;${USER_UID}&quot; ] &amp;&amp; [ &quot;${USER_UID}&quot; != &quot;`id -u ${USER}`&quot; ]; then sed -i -e &quot;s/^${USER}:\\([^:]*\\):[0-9]*:\\([0-9]*\\)/${USER}:\\1:${USER_UID}:\\2/&quot; /etc/passwd fi 那么rootless镜像呢，我看了很兴奋，终于有人做纯正的rootless镜像了，官网的compose事例如下，当时觉得奇怪，都rootless怎么没指定UID，手动加上了basic镜像里面的USER_UID环境变量，发现根本跑不起来，当时在试podman，以为是podman哪里没配对。 1234567891011121314version: &quot;2&quot; services: server: image: docker.gitea.com/gitea:1.23.8-rootless restart: always volumes: - ./data:/var/lib/gitea - ./config:/etc/gitea - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - &quot;3000:3000&quot; - &quot;2222:2222&quot; 然后好奇看了眼官方打包的Dockerfile。 https://github.com/go-gitea/gitea/blob/688da55f543f82265cc7df2bd1cf2bce53188b7a/Dockerfile.rootless#L77 123456# git:git USER 1000:1000 ENV GITEA_WORK_DIR=/var/lib/gitea ENV GITEA_CUSTOM=/var/lib/gitea/custom ENV GITEA_TEMP=/tmp/gitea ENV TMPDIR=/tmp/gitea 谁TMD的rootless镜像指定1000 uid不能更改用的？最后自己拉过来改了dockerfile然后编译 例子3 除了上面那些好歹你能指定一下，有些镜像，你真的不知道他们是什么UID镜像。 postgres这么一个常用镜像，默认uid是999，挂载目录进去因为权限限制的比较死，直接提示权限不够。 虽然postgres库下面提示，可以直接用user去指定uid运行，但是你需要去处理passwd文件。 [!NOTE] As of docker-library/postgres#253⁠, this image supports running as a (mostly) arbitrary user via --user on docker run. As of docker-library/postgres#1018⁠, this is also the case for the Alpine variants. The main caveat to note is that postgres doesn’t care what UID it runs as (as long as the owner of /var/lib/postgresql/data matches), but initdbdoes care (and needs the user to exist in /etc/passwd): 挂载目录没法处理权限这个问题，在k8s或者大型项目中都会使用卷挂载或者pvc之类的块存来避免权限问题，但是在家里挂载目录还是更好维护操作文件，并且好做快照恢复。 总结下来就是，每个容器镜像的实现方式根据开发者的喜好五花八门，层次不齐。但是最终还是因为docker本身没有一个很好且规范的解决方法。 最佳实践 原理 简单来说，把Docker塞进另外的非特权的系统级容器（incus或者lxc）中，通过idmaps 映射host机器的一个组，让容器内要使用的所有用户加入这个容器内的这个组，也就会被自动映射到host的组。另外incus和lxc这类容器默认的root用户也会被映射到主机非常大uid（65534+）的非root用户来确保安全性。 https://linuxcontainers.org/incus/docs/main/userns-idmap/ 那么，在incus容器中的debian，我们就可以随意运行docker容器即使这个应用是root 他获得挂载文件权限的映射路径是 UID:0(Docker) -&gt; UID:0 (IncusDebian) -&gt; GID:3004(IncusDebian) -&gt; GID:3004(TrueNAS Host) 那么即使Docker容器被突破，他能获取到的最大权限是TrueNAS UID:65534+ | GUID:65534+的进程权限和GID:3004的文件权限 具体操作 IDMaps 先添加一个idmaps，在TrueNAS GUI里面 Instance -&gt; Configuration -&gt; Map User/Group IDs, 转到Groups选项，添加你想要映射的组，在图里，我映射了名字rootless 的 GID:3004的图，所以我挂载的所有的路径都会在数据集里面添加rootless的权限。 PS：注意添加权限的时候，请直接添加为Owner Group（所有组），而不是使用NFS/SMB ACL权限的下的权限，并且勾选Apply Group以及递归，如果你要使用ACL权限，请使用POSIX ACL权限 Incus运行Debian以及Docker环境 安装Debian没什么技巧，直接在GUI选择debian安装，然后挂载你需要的路径就行了。 安装完成后，因为GUI还有很多选项没有开发，所以我们需要CLI配置一下 1sudo incus config edit [debian容器名] 你需要在config: 的里面添加添加这三行之后，保存退出（nano操作） 123security.nesting: &quot;true&quot; security.syscalls.intercept.mknod: &quot;true&quot; security.syscalls.intercept.setxattr: &quot;true&quot; 重启容器 1sudo incus restart [debian容器名] 进入容器安装Docker，注意你的网络环境 1234sudo incus shell [debian容器名] apt update &amp;&amp; apt install nano curl -y curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh 添加组，将root加入组rootless 1234groupadd -g 3004 rootless usermod -aG rootless root #重新登陆生效 su - root 然后你再去检查挂载路径，会发现root已经能正常访问。此时你已经能运行大部分容器了 各种容器的应对方法 可配置UID/GID的容器 可以考虑直接配置为UID=0 GID=3004基本上能运行。 不可配置，固定UID的容器 比如postgres这样以UID=999运行的容器，可以模仿root加入组的方式，先找到系统UID=999的应用，例如为这边是netdata 12root@Debian:~# cat /etc/passwd | grep 999 netdata:x:999:997::/var/lib/netdata:/bin/sh 然后让netdata加入rootless组，postgres就能正常运行了 1usermod -aG rootless netdata 多进程不同UID或者即使指定GID 还是没权限 部分容器镜像UID处理方式好像不正常，那么我需要进入这个docker容器，把容器内部的 /etc/group 文件复制出来，然后 ps aux查看进程的UID 在最后一行加上，即让容器的root以及进程ID加入rootless组（没错就是这么阴间，容器内的root有时候会继承不到组权限） 1rootless:x:3004:root,[各种进程ID] 然后改完的文件再在compose里面挂载回 容器内的 /etc/group 1234567services: server: #..... volumes: - ./fake-group-file:/etc/group #..... #..... 总结 总体下来，为了Docker安全性其实付出的维护量还是挺大的，当然如果你不是有需要暴露公网，且不在乎权限，你当然可以root一把嗦，节约自己的时间。 另外可能有人会提到Podman，在安全性上能让容器默认就映射root用户，100%避免了root进程的问题，网络方面也干干净净，但是Podman从历史来看变动太大了，从compose转换的支持到现在的quadlet的写法，以及听说要支持kube apply的yaml写法。让我在主力机器上使用没有很大动力。另外支持Podman的管理工具大部分都还停留在compose。兼容性差的要命，各种option不兼容。","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"},{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"},{"name":"Incus","slug":"Incus","permalink":"https://homelabproject.cc/tags/Incus/"}]},{"title":"Mellanox SN2700 风扇 改造","slug":"Others/Mellanox SN2700 风扇 改造","date":"2025-05-21T15:00:00.000Z","updated":"2025-06-19T04:44:26.917Z","comments":true,"path":"Others/Mellanox SN2700 风扇 改造/","permalink":"https://homelabproject.cc/Others/Mellanox%20SN2700%20%E9%A3%8E%E6%89%87%20%E6%94%B9%E9%80%A0/","excerpt":"","text":"几个月前在闲鱼刷到群里的流量王交换机SN2700，但是结尾是B，问卖家和看datasheet应该是个40G，但是网上搜了一下有人买过SN2100带B结果实际能跑100G。和卖家一顿砍价，5K出头，考虑到价格，符合垃圾佬不一定用得到但是一定要捡漏。 拿到手一看，确实能100G，实际测速也没问题。 剩下遇到的问题就是噪音，虽然SN2700也能想SX60xx系列用命令控制，但是还是会来回拉升，普遍在1w转，满载2w，放在一个小房间在外面也听得到，所以还是得改一下。 SN2700的风扇是模组化的，模组上有个8pin的母座，是sh1.0，建议淘宝买带线做好的，因为sh1.0的端子是真的压不出来。（另外淘宝sh1.0的胶壳普遍质量差，很容易导致脚被压歪，一定要直上直下使用） 线序如下图左到右： [!线序] 12V ｜ 测速 ｜ PWM调速 ｜ GND ｜ 12V ｜ 测速 ｜ PWM调速 ｜ GND 改风扇的关键点： PWM调速 不能接，让风扇直接满速运行 2个测速脚一定要接 风扇转速大于 不等于5K转（低于系统最低阈值，但是不在乎报错是能用的） 风扇只能满速的话，导致4056风扇没得选，因为所有4056都是1-2w转满速，我这里的办法是选择单个4028或者2个4020风扇 单个风扇的话，推荐arctic 4028 6K，转速达标，性能可以。（不要买汕头扇子！永远不是贴标上的型号的翻新），做线的话，把2个测速脚都连风扇的一个脚上即可，其他脚正常链接。 2个4020的话，随便买吧，找个正品的6K左右的3线或者4线风扇，一定带测速，除了pwm调速不接其他练到对应脚就行。然后需要2个隔离柱，太近了因为不是反桨反而效果差，需要隔开点，我这里是3D打印的。（6-2.8-5.8/6-2.8-9.5/外经-内径-长度）每个模块每样2个。最后效果是这样 图一乐的是，我每样做了2个（ 擦电上机，转速都能识别到，但是低于5K的几把风扇过一会就会报转速过低，有空再换掉。 冬天室温很低，温度也没什么问题。夏天可能会高点，但是考虑到我估计连一半端口都用不了，压力也不大。","categories":[],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"https://homelabproject.cc/tags/Hardware/"}]},{"title":"TrueNAS 日志过多","slug":"TrueNAS/TrueNAS 日志过多 debug","date":"2025-04-19T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/TrueNAS 日志过多 debug/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20%E6%97%A5%E5%BF%97%E8%BF%87%E5%A4%9A%20debug/","excerpt":"","text":"1journalctl -q --no-pager --output=json | jq -r &#39;._SYSTEMD_UNIT&#39; | sort | uniq -c | sort -nr 1cat /var/log/middlewared.log","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"TrueNAS  APP（docker）替换官方存储库（repo","slug":"TrueNAS/TrueNAS  APP（docker）替换官方存储库（repo）","date":"2025-02-16T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/TrueNAS  APP（docker）替换官方存储库（repo）/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20%20APP%EF%BC%88docker%EF%BC%89%E6%9B%BF%E6%8D%A2%E5%AE%98%E6%96%B9%E5%AD%98%E5%82%A8%E5%BA%93%EF%BC%88repo%EF%BC%89/","excerpt":"","text":"不想学 我已经镜像到Gitee了，每日定时更新，下面脚本直接执行就能替换，可以放到开机启动，重启或者更新也有效 https://gitee.com/channinghe/truenas-apps 1234567cd /mnt/你放脚本的地方 wget https://file.homelabproject.cc/d/local/Server/Src/truenas/tn-apps-repo-rp.sh chmod +x ./tn-apps-repo-rp.sh bash ./tn-apps-repo-rp.sh --mirrors-url https://gitee.com/channinghe/truenas- 原理 首先官方存储库定义文件在 /usr/lib/python3/dist-packages/middlewared/plugins/catalog/utils.py 这是个不可读文件，所以我们需要用mount的方式 下面操作一律root操作 注意自己备份文件 先到一个放utils.py 替换文件的路径，复制源文件 1cd /mnt/xxx/xxx 1cp /usr/lib/python3/dist-packages/middlewared/plugins/catalog/utils.py ./utils.py 修改URL，你可以nano自己改OFFICIAL_CATALOG_REPO = 'https://github.com/truenas/apps' 内，可以用下面的sed直接替换 1sed -i &#39;s|https://github.com/truenas/apps|https://[github 加速镜像]/truenas/apps|g&#39; ./utils.py 用mount —bind强制覆盖 1mount --bind ./utils.py /usr/lib/python3/dist-packages/middlewared/plugins/catalog/utils.py 最后重启中间价即可 1systemctl restart middlewared","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"Ros 静态路由 到 内网VPN网关慢","slug":"Others/RouterOS 静态路由 到 内网VPN网关慢","date":"2025-02-09T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"Others/RouterOS 静态路由 到 内网VPN网关慢/","permalink":"https://homelabproject.cc/Others/RouterOS%20%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1%20%E5%88%B0%20%E5%86%85%E7%BD%91VPN%E7%BD%91%E5%85%B3%E6%85%A2/","excerpt":"","text":"问题 RouterOS在静态路由 （VPN网段或者 异地网段） 到内网VPN网关时，会遇到体现是第一个响应很慢，但是加载后正常。 排难，iperf3测试带宽没问题，ping延迟也正常的情况。 原因 （懒得写太长，建议看下方链接） 大概就是本地发包转发给内网VPN网关出口后连接无法追踪后判定为”invalid”，通常在防火墙规则最后会有 1chain=forward action=drop connection-state=invalid 导致连接出现”invalid”后直接被drop掉了 解决 在chain=forward action=drop connection-state=invalid 前添加 1chain=forward action=accept connection-state=invalid,new src-address=192.168.2.0/24 dst-address=10.1.1.0/24 in-interface=lan-bridge 即可。 scr-address是你的lan地址，dst-address是你的想要的静态路由的网段（VPN网段或者 异地网段） 就是接受转发到静态路由的网段的”invalid”包 参考 https://forum.mikrotik.com/viewtopic.php?t=171177","categories":[],"tags":[{"name":"RouterOS","slug":"RouterOS","permalink":"https://homelabproject.cc/tags/RouterOS/"}]},{"title":"TrueNAS SCALE NVMe of (nvmetcli)教程","slug":"TrueNAS/TrueNAS SCALE NVMe of (nvmetcli)教程","date":"2025-01-18T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/TrueNAS SCALE NVMe of (nvmetcli)教程/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20SCALE%20NVMe%20of%20(nvmetcli)%E6%95%99%E7%A8%8B/","excerpt":"","text":"更新 2025.05.29 NVME-OF将在25.10 官方支持 视频教程 https://www.bilibili.com/video/BV1AgrYY6EYn 前言 TrueNAS SCALE作为一个存储系统，在NVMe SSD价格越来越香，其实一直缺少高性能的共享存储方式。 对于自带的NFS来说，开启RDMA非常简单只需要给出rdma端口以及加载模块，配置十分简单。SMB direct的话SCALE的内核不支持，所以想要改起来就非常麻烦。 块存储在2025年只有ISCSI对于高负载的应用来说，显得就不够了，无论是tcp还是rdma的NVMeOF在各种方面都优于ISCSI，而且能显著降低CPU的利用率。 其实SCALE的企业版已经支持了NVMeOF 来源：https://www.starwindsoftware.com/blog/iscsi-vs-nvme-of-performance-comparison/ 以及其他的一些对比：https://kb.blockbridge.com/technote/proxmox-iscsi-vs-nvmetcp/# 所以配置NVMeOf迫在眉睫（不是。 入门（ 对于TrueNAS SCALE，因为更新会丢失命令行所有的内容，所以使用nvmetcli 保存配置文件，在更新后只需要加载python环境，pip安装依赖，执行nvmetcli restore config.json 即可恢复nvmeof。 具体操作 配置环境 其实已经有项目写好了脚本，我们就不重复造轮子，简单论述使用方法 https://github.com/democratic-csi/democratic-csi/blob/master/contrib/scale-nvmet-start.sh 1234#切换root su - root #输入密码 curl -o /mnt/池名字/你放脚本/数据集的名字/scale-nvmet-start.sh https://raw.githubusercontent.com/democratic-csi/democratic-csi/refs/heads/master/contrib/scale-nvmet-start.sh 作为例子我这里的脚本放在CPool/myscripts ,后面都要替换我这里的路径 1234#切换root su - root #输入密码 curl -o /mnt/CPool/myscripts/scale-nvmet-start.sh https://raw.githubusercontent.com/democratic-csi/democratic-csi/refs/heads/master/contrib/scale-nvmet-start.sh 然后给予执行权限 1chmod +x /mnt/CPool/myscripts/scale-nvmet-start.sh 需要注意的是这个脚本默认安装最新版本的依赖~~configshell_fb~~ ，这会导致报错，需要指定版本1.1.30 也就是修改这个脚本~~pip install configshell_fb~~ 到~~pip install configshell_fb==1.1.30~~ 然后执行脚本 忽略上面的话，提交修复的PR以及合并，默认就是安装这个版本的了。 1bash /mnt/CPool/myscripts/scale-nvmet-start.sh 如果你网络没问题，那么这个脚本会帮你完成一切的配置 nvmetctl配置NVMeOF 执行下面的命令进入nvmetcli命令行，注意替换CPool/myscripts 1/mnt/CPool/myscripts/nvmet-venv/bin/python /root/.local/bin/nvmetcli 首先要配置一个**subsystems 和** namespaces 1234subsystems/ create nqn=你爱叫什么叫什么 #然后爱叫，我这里叫testof #创建namespaces subsystems/testof/namespaces create nsid=1 然后你可以输入ls查看现在的配置 123456789/&gt; ls o- / ......................................................................................................................... [...] o- hosts ................................................................................................................... [...] o- ports ................................................................................................................... [...] o- subsystems .............................................................................................................. [...] o- testof .............................................................. [version=1.3, allow_any=0, serial=2cb2b6bc4b46653b7680] o- allowed_hosts ....................................................................................................... [...] o- namespaces .......................................................................................................... [...] o- 1 ........................................... [path=(null), uuid=d88dc614-c22d-4942-8a97-5108f210aa1f, grpid=1, disabled] 这里就要修改这个namespaces的path到你的zvol路径,我这里在创建了一个在 ttt池config数据集下的叫nvme-zvol的zvol，那么路径就是固定的**/dev/zvol**+前面的那些，如下 12345/subsystems/testof/namespaces/1 set device path=/dev/zvol/ttt/config/nvme-zvol #同时enable这个namespaces /subsystems/testof/namespaces/1 enable #给testof这个subsystems配置允许任何链接用于测试 /subsystems/testof/ set attr allow_any_host=1 到这里目标配置完毕了。 还要配置一下本机port 1234567#创建一个port /ports create portid=1 #配置addr和param，替换下面ip到你的本机ip /ports/1/ set addr adrfam=ipv4 traddr=192.168.2.140 trtype=rdma trsvcid=4420 /ports/1/ set param inline_data_size=4096 #启动subsystem，没有任何报错就是成功了。如果有报错，看内核日志能看到详细错误 /ports/1/subsystems create testof 最后ls一下 123456789101112131415/&gt; ls o- / .................................................................................... [...] o- hosts .............................................................................. [...] o- ports .............................................................................. [...] | o- 1 ............... [trtype=rdma, traddr=192.168.2.140, trsvcid=4420, inline_data_size=4096] | o- ana_groups ..................................................................... [...] | | o- 1 ................................................................ [state=optimized] | o- referrals ...................................................................... [...] | o- subsystems ..................................................................... [...] | o- testof .................................................................. [...] o- subsystems ......................................................................... [...] o- testof .................... [version=1.3, allow_any=1, serial=2cb2b6bc4b46653b7680] o- allowed_hosts .................................................................. [...] o- namespaces ..................................................................... [...] o- 1 [path=/dev/zvol/ttt/config/nvme-zvol, uuid=d88dc614-c22d-4942-8a97-5108f210aa1f, grpid=1, enabled] 看一下所有的配置，其实直接按照这个配置去配就行了。 我们最后需要保存一下配置为nvmet-config-loaded.json ，记住一定是这个，因为脚本会自动加载这个命名的配置，并且放在脚本的同目录 1saveconfig nvmet-config-loaded.json 然后加载配置，此时就创建完成了 1/mnt/CPool/myscripts/nvmet-venv/bin/python /root/.local/bin/nvmetcli restore nvmet-config-loaded.json 加入自启动 一定要把下面命令加入开机启动，这样就能自动安装环境并且加载配置 1bash /mnt/CPool/myscripts/scale-nvmet-start.sh 测试机器 这里就简述了，根据你自己环境 配置环境 12345apt update &amp;&amp; apt -y install nvme-cli #tcp modprobe nvme_tcp &amp;&amp; echo &quot;nvme_tcp&quot; &gt;&gt; /etc/modules-load.d/nvme_tcp.conf #rmda modprobe nvme_rdma &amp;&amp; echo &quot;nvme_rdma&quot; &gt;&gt; /etc/modules-load.d/nvme_tcp.conf 查找 1nvme discover -t rdma -a 192.168.2.140 -s 4420 链接 1nvme connect -t rdma -n 刚才那个爱叫什么叫什么的 -a 192.168.2.140 -s 4420 查看 12345nvme list #能看到一个model是linux的硬盘，就是nvmeof挂载的了 Node Generic SN Model Namespace Usage Format FW Rev --------------------- --------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- -------- /dev/nvme4n1 /dev/ng4n1 9c5bb3bf618c2999ef73 Linux 1 53.69 GB / 53.69 GB 512 B + 0 B 6.6.44-p 断开链接 1nvme disconnect -n 刚才那个爱叫什么叫什么的 参考： https://www.intel.com/content/dam/support/us/en/documents/network-and-i-o/fabric-products/Config_NVMe_on_Intel_OPA_AN_J78967_v3_0.pdf https://www.reddit.com/r/truenas/comments/1fh3rfl/an_idiots_walkthrough_to_setting_up_nvmeofroce/ https://linbit.com/blog/configuring-highly-available-nvme-of-attached-storage-in-proxmox-ve/ https://kb.blockbridge.com/technote/proxmox-iscsi-vs-nvmetcp/#","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"NFS over RDMA  TrueNAS","slug":"TrueNAS/NFS over RDMA  TrueNAS","date":"2025-01-01T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/NFS over RDMA  TrueNAS/","permalink":"https://homelabproject.cc/TrueNAS/NFS%20over%20RDMA%20%20TrueNAS/","excerpt":"","text":"关键 rpcrdma模块加载 /proc/fs/nfsd/portlist写入rdma端口 你可以直接输入下面命令下载脚本并执行。 1curl -o ./load-nfs-rdma.sh https://file.homelabproject.cc/d/local/Server/Src/rdma-tools/load-nfs-rdma.sh 或者自行粘贴 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/bin/bash LOGFILE=&quot;/var/log/rdma_setup.log&quot; RDMA_PORT=${RDMA_PORT:-20049} # Redirect output to log file exec &gt; &gt;(tee -a &quot;$LOGFILE&quot;) 2&gt;&amp;1 echo &quot;Starting RDMA setup script at $(date)&quot; # Ensure script is run as root if [ &quot;$(id -u)&quot; -ne 0 ]; then echo &quot;This script must be run as root. Please re-run with sudo or as root user.&quot; &gt;&amp;2 exit 1 fi # Check if the rpcrdma module is available if ! modinfo rpcrdma &amp;&gt;/dev/null; then echo &quot;The rpcrdma module is not available on this system. Please ensure it is installed.&quot; &gt;&amp;2 exit 1 fi # Check if the rpcrdma module is loaded if ! lsmod | grep -q &quot;^rpcrdma&quot;; then echo &quot;The rpcrdma module is not loaded, loading it now...&quot; modprobe rpcrdma if [ $? -eq 0 ]; then echo &quot;The rpcrdma module was successfully loaded.&quot; else echo &quot;Failed to load the rpcrdma module. Please check permissions or module availability.&quot; &gt;&amp;2 exit 1 fi else echo &quot;The rpcrdma module is already loaded.&quot; fi # Check if /proc/fs/nfsd/portlist contains the RDMA port if ! grep -q &quot;rdma $RDMA_PORT&quot; /proc/fs/nfsd/portlist; then echo &quot;&#39;rdma $RDMA_PORT&#39; is not present in /proc/fs/nfsd/portlist, adding it now...&quot; echo &quot;rdma $RDMA_PORT&quot; &gt; /proc/fs/nfsd/portlist if [ $? -eq 0 ]; then echo &quot;&#39;rdma $RDMA_PORT&#39; was successfully added to /proc/fs/nfsd/portlist.&quot; else echo &quot;Failed to write to /proc/fs/nfsd/portlist. Please check permissions.&quot; &gt;&amp;2 exit 1 fi else echo &quot;&#39;rdma $RDMA_PORT&#39; is already present in /proc/fs/nfsd/portlist.&quot; fi #echo &quot;RDMA setup script completed successfully at $(date)&quot; 另外可以放入定时任务，自动检测模块加载和端口，确保服务正常","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"SMB 10G 传输文件排难指南","slug":"Others/SMB 10G 传输文件排难指南","date":"2024-10-09T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"Others/SMB 10G 传输文件排难指南/","permalink":"https://homelabproject.cc/Others/SMB%2010G%20%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E6%8E%92%E9%9A%BE%E6%8C%87%E5%8D%97/","excerpt":"","text":"入入入入入入入门教程系列？ 前情提要 很多人在组完NAS，尤其是10G往上局域网或者直连的用户，经常会遇到SMB拷贝一个文件，达不到预计速度，然后问：“我这个NAS系统（万兆网卡）怎么跑不满10G？” ：“？“ 这篇文章会记录一些最基础的排难方式。同时这也能学会如何iperf3和fio命令，以及查看PCIe设备速率。 原因&amp;&amp;分析 当你从挂载的SMB共享拷贝到NAS或者本地，其实中间有很多个环节。 以NAS通过10Gbps网卡直连PC，从本地写入一个文件到NAS为例，能跑到10Gbps需要大致以下的条件： Windows本地磁盘的读取速度大于等于10Gbps Windows本地网卡（上行）速度大于等于10Gbps NAS磁盘写入大于等于10Gbps NAS网卡（下行）大于等于10Gbps Windows以及NAS满足SMB 10Gbps单核性能 其实可以看到，大体就是磁盘速度+网速，但是影响这2个因素的问题又有好多。 还有一种架构，即服务器和PC均在10G交换机下，分别通过10G链接到交换机，虽然多了交换机，但是10G交换机基本不会对环境造成比较大的影响，除了记得在开启PC和NAS 巨帧的同时，交换机也要打开就行。 测试方法 Windows本地磁盘的读取速度大于等于10Gbps？ 测试 https://crystalmark.info/en/software/crystaldiskmark/ 请！这还要教？不行就用各种工具箱里面的。 排难 顺序读写能大于1000MB/s就超过万兆网卡速率了，大部分上面跑不到1000MB/s无非就是SATA SSD和机械硬盘，所以拷贝测试万兆的时候把目标硬盘或者源硬盘选择为NVMe硬盘即可。 Windows本地网卡（上行）速度大于等于10Gbps &amp;&amp; NAS网卡（下行）大于等于10Gbps？ 测试 首先，要用iperf3工具，iperf3的测试是纯网络测试和硬盘速度无关，所以能确定NAS到Windows的网络吞吐是否达标。 下载&amp;&amp;安装 Windows需要自行下载工具 Windows工具下载： （官网）https://iperf.fr/download/windows/iperf-3.1.3-win64.zip （本站）https://alist.homelabproject.cc/d/local/Server/Tools/iperf-3.1.3-win64.zip NAS端安装iperf3 Ubuntu/Debian(PVE/OMV) TrueNAS SCALE/CORE以及 UNRAID自带无需安装。 1apt update &amp;&amp; apt install iperf3 -y CentOS(还有人用？会用得也不用教吧（ 1yum install iperf3-y 群晖等其他没用过给不了教程，这边建议”百度“。 使用 iperf3命令分为服务端和客户端，但是在这里的测试中，在哪边跑都行。但是需要先运行服务端。 服务端 1iperf3 -s -p 52239 解释： -s：代表进入服务端模式 -p：表示指定iperf3服务端口，即使不加会有个默认端口，但是为了避免端口冲突我这里制定了一个，实际可以根据你自己随便给个数字都行 客户端 1iperf -c 192.168.0.123 -p 52239 -P 5 -t 10 -d 解释： -c：代表进入客户端模式，需要链接服务端，后面的参数192.168.0.123，替换成刚刚运行服务端的IP地址即可 -p：指定服务端的端口，需要与上方服务端端口一致即可 -P：代表线程数，即到服务端并行的连接数，这边建议照抄给5就行，一般来说能解决单线程跑不满问题，5个线程跑不满10G基本给多了也没用（注意这里的-P是大写） -t：代表运行时间（单位是秒），10以上基本结果差距不大了，可以自行调整或者抄作业 -d：代表测试双向，发送和接受 注意这些命令在NAS系统(Linux)可以直接执行 在Windows下需要先解压刚刚下载的压缩包，然后Win键+R输入cmd打开命令行 输入cd /盘符 &quot;iperf3解压出来文件夹的路径&quot;，路径可以直接复制windows资源管理器上方的地址 例如： 1cd &quot;C:\\Users\\45251\\Downloads\\iperf-3.1.3-win64&quot; 然后由上文的命令开头 ”iperf3“ 变为 ”.\\iperf3.exe“，后面部分一致 效果如图： 分析结果 执行完命令就能看到最后的结果，实例如下： ① Interval表示时间间隔。 ② Transfer表示时间间隔里面转输的数据量。 ③ Bandwidth是时间间隔里的传输速率。 蓝色框数据是最终的结果，也是5组数据（-P 5 参数）的和，而且因为我们设置了-d，有2组数据分别是发送（sender）和接收(receiver)。除此之外有10组数据，因为我们设置了5个线程+同时测试双向。 可以看到在蓝色框里面我们以及跑满了10Gbps的带宽，证明在网络带宽方面，Windows到NAS端没用问题 排难 错误 如果结果如下 Transfer只有7.53GBytes以及Bandwidth6.47Gbits/sec时，即代表带宽出现了问题。那么就需要进行排难了。 （使用了多线程测速，那么往往是否开启巨帧对结果不会有影响） 从经验来看，iperf3测试无法跑满大概率是因为两边的网卡其中一端的PCIe速率无法满足万兆的需求。因为很多用户使用消费级主板都会把万兆网卡插在非第一槽，而很多厂商有喜欢把他们的来自芯片组的PCIeX1 X4带宽的插槽做成X16的物理插槽来”提升扩展性“，实际只能跑在X1或者X4。但是其实PCIe2.0x4或者PCIe3.0x2以上都已经能满足万兆单口的需求了，大部分仔细查看主板说明书，基本能知道。 关于PCIe速率能跑什么速度，可以参考下面的图，记得乘以0.7左右的损耗，比如PCIe2.0X2写的1.00GB/s，好像理论能跑万兆，实际只能跑在6、700MB/s左右的速度 即使主板写的很清楚，但是也不排除一些阴间情况，比如使用一些延长线，掉PCIe速率，甚至是长度。比如我买到过一根标注M2转PCIeX4的线，只能最高跑在X2（？），导致上2.0卡跑在PCIe2.0X2时就无法跑满万兆带宽。 所以最方便的方法是在系统内查看当前协商速度。 Windows：在“设备管理器”中找到你的网卡，双击点开后，点击详细信息，下来属性框，应该能找到2个参数，如下2个参数 PCIe当前链路宽度代表PCIe槽的长度即PCIeX(16/8/4/1) PCIe当前链路速度代表PCIe(4/3/2).0 例如： 即当前设备运行在PCIe1.0 X4 （需要注意的是这里的数值是16进制，不过网卡都是X4 X8的，就不用管就行） 另外如果是Mellanox的卡，是可以在属性的information里面直接看到速率的 Linux： 先查看一下网卡，找到你自己的网卡，可以看到我这里的是c2:00.0 1sudo lspci | grep -i eth 所以再用下面命令查询c2:00.0的vendor id和device id 1lspci -n | grep -i c2:00.0 最后用这个命令就能查询到当前的的链接速度 1lspci -n -d 8086:10fb -vvv | grep -i width 可以看到我的链接速度(Speed)是5GT/s, 宽度（Width）是X8，链接速度可以直接对应上面的表，就可以直接现在这张卡跑在PCIe2.0x8，带宽是没问题的 查询的步骤就分为这2部分，如果有一边低于PCIe2.0x4或者PCIe3.0x2的话，就代表无法跑满万兆，需要更换一个PCIe位置。 NAS磁盘写入大于等于10Gbps？ 测试 这里要用到Fio 是一个功能强大的 I/O 压力测试工具，可以对硬盘进行顺序读写测试。 安装 Ubuntu/Debian(PVE/OMV) 1apt update &amp;&amp; apt install fio -y CentOS(还有人用？会用得也不用教吧（ 1yum install iperf3-y 使用 先用cd 命令进入你要测试的文件系统或者阵列 1cd [需要测试的路径] 然后使用下面命令创建一个FIO测试文件的配置文件 1234567891011cat &gt; test_disk_read.fio &lt;&lt;EOF [seq-read] name=Sequential Read Test ioengine=libaio iodepth=8 rw=read bs=1M size=10G numjobs=16 runtime=60 EOF 1234567891011cat &gt; test_disk_write.fio &lt;&lt;EOF [seq-write] name=Sequential Write Test ioengine=libaio iodepth=8 rw=write bs=1M size=10G numjobs=16 runtime=60 EOF （照抄作业就行，关于具体参数，日后更新吧。。） 开始测试 测试读取 1fio test_disk_read.fio 测试写入 1fio test_disk_write.fio （测试需要一定时间） 最后应该我们会得到这样一个测试结果，READ代表读取，WRITE代表写入，如果2者均超过1000MiB/s，那么在万兆传输这一个问题应该不会产生瓶颈。 123Run status group 0 (all jobs): READ: bw=1072MiB/s (1124MB/s), 1072MiB/s-1072MiB/s (1124MB/s-1124MB/s), io=10.0GiB (10.7GB), run=9553-9553msec WRITE: bw=2003MiB/s (2100MB/s), 2003MiB/s-2003MiB/s (2100MB/s-2100MB/s), io=10.0GiB (10.7GB), run=5113-5113msec 排难 其实这一部分无非也就是你机械阵列的问题，需要优化你的阵列速度，那不同文件系统就相差甚远，需要自己进行调整，这里只能帮忙定位问题。 Windows以及NAS满足SMB 10Gbps单核性能？ 其实这也是非常简单的，Windows传文件的时候打开任务管理，查看在跑不满的时候，是否有一个核心跑满了，那就证明单核性能瓶颈了。（2023年应该不会有U单核这样瓶颈吧） 总结 这篇文件给出了一个最基础问题的定位方式，以及命令简单的理解，基本能解决97%（？）的问题，后面可以考虑再更新一下FIO命令的详细解释，不过这东西网上一堆，这里给出教程只是希望你在着急使用时能快速学会，不会因为看一遍文章，要去翻好几篇才能看懂。 引用&amp;&amp;备注 https://en.wikipedia.org/wiki/PCI_Express “Icon made by Nikita Golubev from www.flaticon.com” “Icon made by Pixel perfect from www.flaticon.com”","categories":[],"tags":[{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"}]},{"title":"Proxmox VE (PVE) CT LXC备份失败","slug":"Proxmox/Proxmox VE (PVE) CT LXC备份失败","date":"2024-04-10T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"Proxmox/Proxmox VE (PVE) CT LXC备份失败/","permalink":"https://homelabproject.cc/Proxmox/Proxmox%20VE%20(PVE)%20CT%20LXC%E5%A4%87%E4%BB%BD%E5%A4%B1%E8%B4%A5/","excerpt":"","text":"问题 在PVE中使用NFS作为存储经常会遇到权限问题，我这里使用FakeNAS~~(TrueNAS SCALE)~~ NFS的共享作为虚拟机的备份，但是会发现非特权容器都会出现备份失败的情况具体情况如下 123456789101112131415161718192021222324252627INFO: starting new backup job: vzdump 112 --notes-template &#39;{{guestname}}&#39; --storage vm_omv --remove 0 --mode snapshot --compress zstd --node serverhub INFO: Starting Backup of VM 112 (lxc) INFO: Backup started at 2023-06-25 14:50:12 INFO: status = running INFO: CT Name: Nginx INFO: including mount point rootfs (&#39;/&#39;) in backup INFO: mode failure - some volumes do not support snapshots INFO: trying &#39;suspend&#39; mode instead INFO: backup mode: suspend INFO: ionice priority: 7 INFO: CT Name: Nginx INFO: including mount point rootfs (&#39;/&#39;) in backup INFO: temporary directory is on NFS, disabling xattr and acl support, consider configuring a local tmpdir via /etc/vzdump.conf INFO: starting first sync /proc/2982556/root/ to /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp INFO: first sync finished - transferred 1.29G bytes in 61s INFO: suspending guest INFO: starting final sync /proc/2982556/root/ to /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp INFO: final sync finished - transferred 1.05M bytes in 1s INFO: resuming guest INFO: guest is online again after 1 seconds INFO: creating vzdump archive &#39;/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tar.zst&#39; INFO: tar: /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp: Cannot open: Permission denied INFO: tar: Error is not recoverable: exiting now ERROR: Backup of VM 112 failed - command &#39;set -o pipefail &amp;&amp; lxc-usernsexec -m u:0:100000:65536 -m g:0:100000:65536 -- tar cpf - --totals --one-file-system -p --sparse --numeric-owner --acls --xattrs &#39;--xattrs-include=user.*&#39; &#39;--xattrs-include=security.capability&#39; &#39;--warning=no-file-ignored&#39; &#39;--warning=no-xattr-write&#39; --one-file-system &#39;--warning=no-file-ignored&#39; &#39;--directory=/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp&#39; ./etc/vzdump/pct.conf ./etc/vzdump/pct.fw &#39;--directory=/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp&#39; --no-anchored &#39;--exclude=lost+found&#39; --anchored &#39;--exclude=./tmp/?*&#39; &#39;--exclude=./var/tmp/?*&#39; &#39;--exclude=./var/run/?*.pid&#39; . | zstd --rsyncable &#39;--threads=1&#39; &gt;/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tar.dat&#39; failed: exit code 2 INFO: Failed at 2023-06-25 14:51:25 INFO: Backup job finished with errors TASK ERROR: job errors 可以看到报错中提示存储NFS 提示权限限制 原因是因为LXC使用的是非root用户，而我在TrueNAS使用Maproot User会仅限于客户端（PVE）的root用户映射到NFS服务端（TrueNAS）的root用户的权限，导致权限出错。 解决方法 更改备份的临时目录到系统盘就能轻松秒杀，但是我的系统盘是16G傲腾，无法支撑临时备份，所以不行。 1echo &quot;tmpdir: /tmp&quot; &gt;&gt; /etc/vzdump.conf 将Maproot的选项更改到Mapall，即 将“NFS服务器将在客户端访问共享时将所有用户的权限映射为root用户的权限，但仅限于root用户”更改为“NFS服务器将在客户端访问共享时将所有用户的权限映射为root用户的权限。”。让客户端（PVE）非root权限(非特权)的LXC容器也能映射为NFS服务端（TrueNAS）root用户。","categories":[],"tags":[{"name":"ProxmoxVE","slug":"ProxmoxVE","permalink":"https://homelabproject.cc/tags/ProxmoxVE/"}]},{"title":"TrueNAS SCALE（Network UPS Tools） 使用CyberPower UPS 10-20小时 断连","slug":"TrueNAS/TrueNAS CyberPower NUT (Network UPS Tools) 断连","date":"2023-05-21T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"TrueNAS/TrueNAS CyberPower NUT (Network UPS Tools) 断连/","permalink":"https://homelabproject.cc/TrueNAS/TrueNAS%20CyberPower%20NUT%20(Network%20UPS%20Tools)%20%E6%96%AD%E8%BF%9E/","excerpt":"","text":"在/usr/nut/ups.conf 或者WebUI-系统设置-服务-UPS-Auxiliary Parameters (ups.conf)中加入 1pollonly = &quot;enabled&quot; 保存，重启ups服务即可 图示： 参考来源：https://github.com/networkupstools/nut/issues/1029","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"文章归档","slug":"Others/Posts","date":"1999-12-31T15:00:00.000Z","updated":"2025-06-19T04:44:26.917Z","comments":true,"path":"/posts/","permalink":"https://homelabproject.cc/posts/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"友链","slug":"Others/友链","date":"1900-12-31T15:00:00.000Z","updated":"2025-06-19T04:44:26.921Z","comments":true,"path":"Others/友链/","permalink":"https://homelabproject.cc/Others/%E5%8F%8B%E9%93%BE/","excerpt":"","text":"","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"SelfHosted","slug":"SelfHosted","permalink":"https://homelabproject.cc/tags/SelfHosted/"},{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"},{"name":"Docker","slug":"Docker","permalink":"https://homelabproject.cc/tags/Docker/"},{"name":"Infra","slug":"Infra","permalink":"https://homelabproject.cc/tags/Infra/"},{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"},{"name":"Incus","slug":"Incus","permalink":"https://homelabproject.cc/tags/Incus/"},{"name":"Hardware","slug":"Hardware","permalink":"https://homelabproject.cc/tags/Hardware/"},{"name":"RouterOS","slug":"RouterOS","permalink":"https://homelabproject.cc/tags/RouterOS/"},{"name":"ProxmoxVE","slug":"ProxmoxVE","permalink":"https://homelabproject.cc/tags/ProxmoxVE/"}]}