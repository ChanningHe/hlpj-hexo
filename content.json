{"meta":{"title":"HomeLabProject","subtitle":"","description":"","author":"Channing He","url":"https://homelabproject.cc","root":"/"},"pages":[{"title":"","date":"2025-05-23T11:59:07.782Z","updated":"2025-05-23T11:59:07.782Z","comments":true,"path":"custom.css","permalink":"https://homelabproject.cc/custom.css","excerpt":"","text":":root{--gutter:25px;--radius:13px;--color-primary:#1c6c36;--color2:#ff761e;--color3:#ffb900;--color4:#33d57a;--color5:#00dbff;--color6:#1a98ff;--color7:#9090ff;--color-primary-bg:rgba(20, 103, 10, 0.15);--color2-bg:rgba(255,118,30,0.15);--color3-bg:rgba(255,185,0,0.15);--color4-bg:rgba(51,213,122,0.15);--color5-bg:rgba(0,219,255,0.15);--color6-bg:rgba(26,152,255,0.15);--color7-bg:rgba(144,144,255,0.15);--color-shadow:rgba(161, 177, 204, 0.4)}"}],"posts":[{"title":"Mellanox SN2700 风扇 改造","slug":"Others/Mellanox SN2700 风扇 改造","date":"2025-05-21T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2025/05/21/Others/Mellanox SN2700 风扇 改造/","permalink":"https://homelabproject.cc/2025/05/21/Others/Mellanox%20SN2700%20%E9%A3%8E%E6%89%87%20%E6%94%B9%E9%80%A0/","excerpt":"","text":"几个月前在闲鱼刷到群里的流量王交换机SN2700，但是结尾是B，问卖家和看datasheet应该是个40G，但是网上搜了一下有人买过SN2100带B结果实际能跑100G。和卖家一顿砍价，5K出头，考虑到价格，符合垃圾佬不一定用得到但是一定要捡漏。 拿到手一看，确实能100G，实际测速也没问题。 剩下遇到的问题就是噪音，虽然SN2700也能想SX60xx系列用命令控制，但是还是会来回拉升，普遍在1w转，满载2w，放在一个小房间在外面也听得到，所以还是得改一下。 SN2700的风扇是模组化的，模组上有个8pin的母座，是sh1.0，建议淘宝买带线做好的，因为sh1.0的端子是真的压不出来。（另外淘宝sh1.0的胶壳普遍质量差，很容易导致脚被压歪，一定要直上直下使用）线序如下图左到右： 12V ｜ 测速 ｜ PWM调速 ｜ GND ｜12V ｜ 测速 ｜ PWM调速 ｜ GND 改风扇的关键点： PWM调速 不能接，让风扇直接满速运行 2个测速脚一定要接 风扇转速大于 不等于5K转（低于系统最低阈值，但是不在乎报错是能用的） 风扇只能满速的话，导致4056风扇没得选，因为所有4056都是1-2w转满速，我这里的办法是选择单个4028或者2个4020风扇 单个风扇的话，推荐arctic 4028 6K，转速达标，性能可以。（不要买汕头扇子！永远不是贴标上的型号的翻新），做线的话，把2个测速脚都连风扇的一个脚上即可，其他脚正常链接。 2个4020的话，随便买吧，找个正品的6K左右的3线或者4线风扇，一定带测速，除了pwm调速不接其他练到对应脚就行。然后需要2个隔离柱，太近了因为不是反桨反而效果差，需要隔开点，我这里是3D打印的。（6-2.8-5.8&#x2F;6-2.8-9.5&#x2F;外经-内径-长度）每个模块每样2个。最后效果是这样 图一乐的是，我每样做了2个（ 擦电上机，转速都能识别到，但是低于5K的几把风扇过一会就会报转速过低，有空再换掉。 冬天室温很低，温度也没什么问题。夏天可能会高点，但是考虑到我估计连一半端口都用不了，压力也不大。","categories":[],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"https://homelabproject.cc/tags/Hardware/"}]},{"title":"TrueNAS 日志过多","slug":"TrueNAS/TrueNAS-debug","date":"2025-04-19T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2025/04/19/TrueNAS/TrueNAS-debug/","permalink":"https://homelabproject.cc/2025/04/19/TrueNAS/TrueNAS-debug/","excerpt":"","text":"1journalctl -q --no-pager --output=json | jq -r &#x27;._SYSTEMD_UNIT&#x27; | sort | uniq -c | sort -nr 1cat /var/log/middlewared.log","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"Ros 静态路由 到 内网VPN网关慢","slug":"Others/RouterOS 静态路由 到 内网VPN网关慢","date":"2025-02-09T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2025/02/09/Others/RouterOS 静态路由 到 内网VPN网关慢/","permalink":"https://homelabproject.cc/2025/02/09/Others/RouterOS%20%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1%20%E5%88%B0%20%E5%86%85%E7%BD%91VPN%E7%BD%91%E5%85%B3%E6%85%A2/","excerpt":"","text":"问题RouterOS在静态路由 （VPN网段或者 异地网段） 到内网VPN网关时，会遇到体现是第一个响应很慢，但是加载后正常。 排难，iperf3测试带宽没问题，ping延迟也正常的情况。 原因（懒得写太长，建议看下方链接） 大概就是本地发包转发给内网VPN网关出口后连接无法追踪后判定为”invalid”，通常在防火墙规则最后会有 1chain=forward action=drop connection-state=invalid 导致连接出现”invalid”后直接被drop掉了 解决在chain=forward action=drop connection-state=invalid 前添加 1chain=forward action=accept connection-state=invalid,new src-address=192.168.2.0/24 dst-address=10.1.1.0/24 in-interface=lan-bridge 即可。 scr-address是你的lan地址，dst-address是你的想要的静态路由的网段（VPN网段或者 异地网段） 就是接受转发到静态路由的网段的”invalid”包 参考https://forum.mikrotik.com/viewtopic.php?t=171177","categories":[],"tags":[{"name":"RouterOS","slug":"RouterOS","permalink":"https://homelabproject.cc/tags/RouterOS/"}]},{"title":"TrueNAS SCALE NVMe of (nvmetcli)教程","slug":"TrueNAS/TrueNAS SCALE NVMe of (nvmetcli)教程","date":"2025-01-18T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2025/01/18/TrueNAS/TrueNAS SCALE NVMe of (nvmetcli)教程/","permalink":"https://homelabproject.cc/2025/01/18/TrueNAS/TrueNAS%20SCALE%20NVMe%20of%20(nvmetcli)%E6%95%99%E7%A8%8B/","excerpt":"","text":"视频教程https://www.bilibili.com/video/BV1AgrYY6EYn 前言TrueNAS SCALE作为一个存储系统，在NVMe SSD价格越来越香，其实一直缺少高性能的共享存储方式。 对于自带的NFS来说，开启RDMA非常简单只需要给出rdma端口以及加载模块，配置十分简单。SMB direct的话SCALE的内核不支持，所以想要改起来就非常麻烦。 块存储在2025年只有ISCSI对于高负载的应用来说，显得就不够了，无论是tcp还是rdma的NVMeOF在各种方面都优于ISCSI，而且能显著降低CPU的利用率。 其实SCALE的企业版已经支持了NVMeOF 来源：https://www.starwindsoftware.com/blog/iscsi-vs-nvme-of-performance-comparison/ 以及其他的一些对比：https://kb.blockbridge.com/technote/proxmox-iscsi-vs-nvmetcp/# 所以配置NVMeOf迫在眉睫（不是。 入门（对于TrueNAS SCALE，因为更新会丢失命令行所有的内容，所以使用nvmetcli 保存配置文件，在更新后只需要加载python环境，pip安装依赖，执行nvmetcli restore config.json 即可恢复nvmeof。 具体操作配置环境其实已经有项目写好了脚本，我们就不重复造轮子，简单论述使用方法 https://github.com/democratic-csi/democratic-csi/blob/master/contrib/scale-nvmet-start.sh 1234#切换rootsu - root#输入密码curl -o /mnt/池名字/你放脚本/数据集的名字/scale-nvmet-start.sh https://raw.githubusercontent.com/democratic-csi/democratic-csi/refs/heads/master/contrib/scale-nvmet-start.sh 作为例子我这里的脚本放在CPool/myscripts ,后面都要替换我这里的路径 1234#切换rootsu - root#输入密码curl -o /mnt/CPool/myscripts/scale-nvmet-start.sh https://raw.githubusercontent.com/democratic-csi/democratic-csi/refs/heads/master/contrib/scale-nvmet-start.sh 然后给予执行权限 1chmod +x /mnt/CPool/myscripts/scale-nvmet-start.sh 需要注意的是这个脚本默认安装最新版本的依赖~~~~&#96;configshell_fb&#96; ，这会导致报错，需要指定版本1.1.30 也就是修改这个脚本~~~~&#96;pip install configshell_fb&#96; 到~~~~&#96;pip install configshell_fb&#x3D;&#x3D;1.1.30&#96; 然后执行脚本 忽略上面的话，提交修复的PR以及合并，默认就是安装这个版本的了。 1bash /mnt/CPool/myscripts/scale-nvmet-start.sh 如果你网络没问题，那么这个脚本会帮你完成一切的配置 nvmetctl配置NVMeOF执行下面的命令进入nvmetcli命令行，注意替换CPool/myscripts 1/mnt/CPool/myscripts/nvmet-venv/bin/python /root/.local/bin/nvmetcli 首先要配置一个**subsystems 和** namespaces 1234subsystems/ create nqn=你爱叫什么叫什么#然后爱叫，我这里叫testof#创建namespacessubsystems/testof/namespaces create nsid=1 然后你可以输入ls查看现在的配置 123456789/&gt; lso- / ......................................................................................................................... [...] o- hosts ................................................................................................................... [...] o- ports ................................................................................................................... [...] o- subsystems .............................................................................................................. [...] o- testof .............................................................. [version=1.3, allow_any=0, serial=2cb2b6bc4b46653b7680] o- allowed_hosts ....................................................................................................... [...] o- namespaces .......................................................................................................... [...] o- 1 ........................................... [path=(null), uuid=d88dc614-c22d-4942-8a97-5108f210aa1f, grpid=1, disabled] 这里就要修改这个namespaces的path到你的zvol路径,我这里在创建了一个在 ttt池config数据集下的叫nvme-zvol的zvol，那么路径就是固定的**&#x2F;dev&#x2F;zvol**+前面的那些，如下 12345/subsystems/testof/namespaces/1 set device path=/dev/zvol/ttt/config/nvme-zvol#同时enable这个namespaces/subsystems/testof/namespaces/1 enable #给testof这个subsystems配置允许任何链接用于测试/subsystems/testof/ set attr allow_any_host=1 到这里目标配置完毕了。 还要配置一下本机port 1234567#创建一个port/ports create portid=1#配置addr和param，替换下面ip到你的本机ip/ports/1/ set addr adrfam=ipv4 traddr=192.168.2.140 trtype=rdma trsvcid=4420 /ports/1/ set param inline_data_size=4096#启动subsystem，没有任何报错就是成功了。如果有报错，看内核日志能看到详细错误/ports/1/subsystems create testof 最后ls一下 123456789101112131415/&gt; lso- / .................................................................................... [...] o- hosts .............................................................................. [...] o- ports .............................................................................. [...] | o- 1 ............... [trtype=rdma, traddr=192.168.2.140, trsvcid=4420, inline_data_size=4096] | o- ana_groups ..................................................................... [...] | | o- 1 ................................................................ [state=optimized] | o- referrals ...................................................................... [...] | o- subsystems ..................................................................... [...] | o- testof .................................................................. [...] o- subsystems ......................................................................... [...] o- testof .................... [version=1.3, allow_any=1, serial=2cb2b6bc4b46653b7680] o- allowed_hosts .................................................................. [...] o- namespaces ..................................................................... [...] o- 1 [path=/dev/zvol/ttt/config/nvme-zvol, uuid=d88dc614-c22d-4942-8a97-5108f210aa1f, grpid=1, enabled] 看一下所有的配置，其实直接按照这个配置去配就行了。 我们最后需要保存一下配置为nvmet-config-loaded.json ，记住一定是这个，因为脚本会自动加载这个命名的配置，并且放在脚本的同目录 1saveconfig nvmet-config-loaded.json 然后加载配置，此时就创建完成了 1/mnt/CPool/myscripts/nvmet-venv/bin/python /root/.local/bin/nvmetcli restore nvmet-config-loaded.json 加入自启动一定要把下面命令加入开机启动，这样就能自动安装环境并且加载配置 1bash /mnt/CPool/myscripts/scale-nvmet-start.sh 测试机器这里就简述了，根据你自己环境 配置环境 12345apt update &amp;&amp; apt -y install nvme-cli#tcpmodprobe nvme_tcp &amp;&amp; echo &quot;nvme_tcp&quot; &gt;&gt; /etc/modules-load.d/nvme_tcp.conf#rmdamodprobe nvme_rdma &amp;&amp; echo &quot;nvme_rdma&quot; &gt;&gt; /etc/modules-load.d/nvme_tcp.conf 查找 1nvme discover -t rdma -a 192.168.2.140 -s 4420 链接 1nvme connect -t rdma -n 刚才那个爱叫什么叫什么的 -a 192.168.2.140 -s 4420 查看 12345nvme list#能看到一个model是linux的硬盘，就是nvmeof挂载的了Node Generic SN Model Namespace Usage Format FW Rev --------------------- --------------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------/dev/nvme4n1 /dev/ng4n1 9c5bb3bf618c2999ef73 Linux 1 53.69 GB / 53.69 GB 512 B + 0 B 6.6.44-p 断开链接 1nvme disconnect -n 刚才那个爱叫什么叫什么的 参考： https://www.intel.com/content/dam/support/us/en/documents/network-and-i-o/fabric-products/Config_NVMe_on_Intel_OPA_AN_J78967_v3_0.pdf https://www.reddit.com/r/truenas/comments/1fh3rfl/an_idiots_walkthrough_to_setting_up_nvmeofroce&#x2F; https://linbit.com/blog/configuring-highly-available-nvme-of-attached-storage-in-proxmox-ve/ https://kb.blockbridge.com/technote/proxmox-iscsi-vs-nvmetcp/#","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]},{"title":"SMB 10G 传输文件排难指南","slug":"Others/SMB 10G 传输文件排难指南","date":"2024-10-09T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2024/10/09/Others/SMB 10G 传输文件排难指南/","permalink":"https://homelabproject.cc/2024/10/09/Others/SMB%2010G%20%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E6%8E%92%E9%9A%BE%E6%8C%87%E5%8D%97/","excerpt":"","text":"入入入入入入入门教程系列？ 前情提要很多人在组完NAS，尤其是10G往上局域网或者直连的用户，经常会遇到SMB拷贝一个文件，达不到预计速度，然后问：“我这个NAS系统（万兆网卡）怎么跑不满10G？” ：“？“ 这篇文章会记录一些最基础的排难方式。同时这也能学会如何iperf3和fio命令，以及查看PCIe设备速率。 原因&amp;&amp;分析当你从挂载的SMB共享拷贝到NAS或者本地，其实中间有很多个环节。 以NAS通过10Gbps网卡直连PC，从本地写入一个文件到NAS为例，能跑到10Gbps需要大致以下的条件： Windows本地磁盘的读取速度大于等于10Gbps Windows本地网卡（上行）速度大于等于10Gbps NAS磁盘写入大于等于10Gbps NAS网卡（下行）大于等于10Gbps Windows以及NAS满足SMB 10Gbps单核性能 其实可以看到，大体就是磁盘速度+网速，但是影响这2个因素的问题又有好多。 还有一种架构，即服务器和PC均在10G交换机下，分别通过10G链接到交换机，虽然多了交换机，但是10G交换机基本不会对环境造成比较大的影响，除了记得在开启PC和NAS 巨帧的同时，交换机也要打开就行。 测试方法Windows本地磁盘的读取速度大于等于10Gbps？测试https://crystalmark.info/en/software/crystaldiskmark/ 请！这还要教？不行就用各种工具箱里面的。 排难顺序读写能大于1000MB&#x2F;s就超过万兆网卡速率了，大部分上面跑不到1000MB&#x2F;s无非就是SATA SSD和机械硬盘，所以拷贝测试万兆的时候把目标硬盘或者源硬盘选择为NVMe硬盘即可。 Windows本地网卡（上行）速度大于等于10Gbps &amp;&amp; NAS网卡（下行）大于等于10Gbps？测试首先，要用iperf3工具，iperf3的测试是纯网络测试和硬盘速度无关，所以能确定NAS到Windows的网络吞吐是否达标。 下载&amp;&amp;安装 Windows需要自行下载工具 Windows工具下载： （官网）https://iperf.fr/download/windows/iperf-3.1.3-win64.zip （本站）https://alist.homelabproject.cc/d/local/Server/Tools/iperf-3.1.3-win64.zip NAS端安装iperf3 Ubuntu&#x2F;Debian(PVE&#x2F;OMV) TrueNAS SCALE&#x2F;CORE以及 UNRAID自带无需安装。 1apt update &amp;&amp; apt install iperf3 -y CentOS(还有人用？会用得也不用教吧（ 1yum install iperf3-y 群晖等其他没用过给不了教程，这边建议”百度“。 使用 iperf3命令分为服务端和客户端，但是在这里的测试中，在哪边跑都行。但是需要先运行服务端。 服务端 1iperf3 -s -p 52239 解释： -s：代表进入服务端模式 -p：表示指定iperf3服务端口，即使不加会有个默认端口，但是为了避免端口冲突我这里制定了一个，实际可以根据你自己随便给个数字都行 客户端 1iperf -c 192.168.0.123 -p 52239 -P 5 -t 10 -d 解释： -c：代表进入客户端模式，需要链接服务端，后面的参数192.168.0.123，替换成刚刚运行服务端的IP地址即可 -p：指定服务端的端口，需要与上方服务端端口一致即可 -P：代表线程数，即到服务端并行的连接数，这边建议照抄给5就行，一般来说能解决单线程跑不满问题，5个线程跑不满10G基本给多了也没用（注意这里的-P是大写） -t：代表运行时间（单位是秒），10以上基本结果差距不大了，可以自行调整或者抄作业 -d：代表测试双向，发送和接受 注意这些命令在NAS系统(Linux)可以直接执行 在Windows下需要先解压刚刚下载的压缩包，然后Win键+R输入cmd打开命令行 输入cd /盘符 &quot;iperf3解压出来文件夹的路径&quot;，路径可以直接复制windows资源管理器上方的地址 例如： 1cd &quot;C:\\Users\\45251\\Downloads\\iperf-3.1.3-win64&quot; 然后由上文的命令开头 ”iperf3“ 变为 ”.\\iperf3.exe“，后面部分一致 效果如图： 分析结果 执行完命令就能看到最后的结果，实例如下： ① Interval表示时间间隔。 ② Transfer表示时间间隔里面转输的数据量。 ③ Bandwidth是时间间隔里的传输速率。 蓝色框数据是最终的结果，也是5组数据（-P 5 参数）的和，而且因为我们设置了-d，有2组数据分别是发送（sender）和接收(receiver)。除此之外有10组数据，因为我们设置了5个线程+同时测试双向。 可以看到在蓝色框里面我们以及跑满了10Gbps的带宽，证明在网络带宽方面，Windows到NAS端没用问题 排难 错误 如果结果如下 Transfer只有7.53GBytes以及Bandwidth6.47Gbits&#x2F;sec时，即代表带宽出现了问题。那么就需要进行排难了。 （使用了多线程测速，那么往往是否开启巨帧对结果不会有影响） 从经验来看，iperf3测试无法跑满大概率是因为两边的网卡其中一端的PCIe速率无法满足万兆的需求。因为很多用户使用消费级主板都会把万兆网卡插在非第一槽，而很多厂商有喜欢把他们的来自芯片组的PCIeX1 X4带宽的插槽做成X16的物理插槽来”提升扩展性“，实际只能跑在X1或者X4。但是其实PCIe2.0x4或者PCIe3.0x2以上都已经能满足万兆单口的需求了，大部分仔细查看主板说明书，基本能知道。 关于PCIe速率能跑什么速度，可以参考下面的图，记得乘以0.7左右的损耗，比如PCIe2.0X2写的1.00GB&#x2F;s，好像理论能跑万兆，实际只能跑在6、700MB&#x2F;s左右的速度 即使主板写的很清楚，但是也不排除一些阴间情况，比如使用一些延长线，掉PCIe速率，甚至是长度。比如我买到过一根标注M2转PCIeX4的线，只能最高跑在X2（？），导致上2.0卡跑在PCIe2.0X2时就无法跑满万兆带宽。 所以最方便的方法是在系统内查看当前协商速度。 Windows：在“设备管理器”中找到你的网卡，双击点开后，点击详细信息，下来属性框，应该能找到2个参数，如下2个参数 PCIe当前链路宽度代表PCIe槽的长度即PCIeX(16&#x2F;8&#x2F;4&#x2F;1) PCIe当前链路速度代表PCIe(4&#x2F;3&#x2F;2).0 例如： 即当前设备运行在PCIe1.0 X4 （需要注意的是这里的数值是16进制，不过网卡都是X4 X8的，就不用管就行） 另外如果是Mellanox的卡，是可以在属性的information里面直接看到速率的 Linux： 先查看一下网卡，找到你自己的网卡，可以看到我这里的是c2:00.0 1sudo lspci | grep -i eth 所以再用下面命令查询c2:00.0的vendor id和device id 1lspci -n | grep -i c2:00.0 最后用这个命令就能查询到当前的的链接速度 1lspci -n -d 8086:10fb -vvv | grep -i width 可以看到我的链接速度(Speed)是5GT&#x2F;s, 宽度（Width）是X8，链接速度可以直接对应上面的表，就可以直接现在这张卡跑在PCIe2.0x8，带宽是没问题的 查询的步骤就分为这2部分，如果有一边低于PCIe2.0x4或者PCIe3.0x2的话，就代表无法跑满万兆，需要更换一个PCIe位置。 NAS磁盘写入大于等于10Gbps？测试这里要用到Fio 是一个功能强大的 I&#x2F;O 压力测试工具，可以对硬盘进行顺序读写测试。 安装 Ubuntu&#x2F;Debian(PVE&#x2F;OMV) 1apt update &amp;&amp; apt install fio -y CentOS(还有人用？会用得也不用教吧（ 1yum install iperf3-y 使用 先用cd 命令进入你要测试的文件系统或者阵列 1cd [需要测试的路径] 然后使用下面命令创建一个FIO测试文件的配置文件 123456789101112cat &gt; test_disk_read.fio &lt;&lt;EOF[seq-read]name=Sequential Read Testioengine=libaioiodepth=8rw=readbs=1Msize=10Gnumjobs=16runtime=60EOF 1234567891011cat &gt; test_disk_write.fio &lt;&lt;EOF[seq-write]name=Sequential Write Testioengine=libaioiodepth=8rw=writebs=1Msize=10Gnumjobs=16runtime=60EOF （照抄作业就行，关于具体参数，日后更新吧。。） 开始测试 测试读取 1fio test_disk_read.fio 测试写入 1fio test_disk_write.fio （测试需要一定时间） 最后应该我们会得到这样一个测试结果，READ代表读取，WRITE代表写入，如果2者均超过1000MiB&#x2F;s，那么在万兆传输这一个问题应该不会产生瓶颈。 123Run status group 0 (all jobs): READ: bw=1072MiB/s (1124MB/s), 1072MiB/s-1072MiB/s (1124MB/s-1124MB/s), io=10.0GiB (10.7GB), run=9553-9553msec WRITE: bw=2003MiB/s (2100MB/s), 2003MiB/s-2003MiB/s (2100MB/s-2100MB/s), io=10.0GiB (10.7GB), run=5113-5113msec 排难其实这一部分无非也就是你机械阵列的问题，需要优化你的阵列速度，那不同文件系统就相差甚远，需要自己进行调整，这里只能帮忙定位问题。 Windows以及NAS满足SMB 10Gbps单核性能？其实这也是非常简单的，Windows传文件的时候打开任务管理，查看在跑不满的时候，是否有一个核心跑满了，那就证明单核性能瓶颈了。（2023年应该不会有U单核这样瓶颈吧） 总结这篇文件给出了一个最基础问题的定位方式，以及命令简单的理解，基本能解决97%（？）的问题，后面可以考虑再更新一下FIO命令的详细解释，不过这东西网上一堆，这里给出教程只是希望你在着急使用时能快速学会，不会因为看一遍文章，要去翻好几篇才能看懂。 引用&amp;&amp;备注https://en.wikipedia.org/wiki/PCI_Express “Icon made by Nikita Golubev from www.flaticon.com“ “Icon made by Pixel perfect from www.flaticon.com“","categories":[],"tags":[{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"}]},{"title":"Proxmox VE (PVE) CT LXC备份失败","slug":"Proxmox/Proxmox VE (PVE) CT LXC备份失败","date":"2024-04-10T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2024/04/10/Proxmox/Proxmox VE (PVE) CT LXC备份失败/","permalink":"https://homelabproject.cc/2024/04/10/Proxmox/Proxmox%20VE%20(PVE)%20CT%20LXC%E5%A4%87%E4%BB%BD%E5%A4%B1%E8%B4%A5/","excerpt":"","text":"问题在PVE中使用NFS作为存储经常会遇到权限问题，我这里使用FakeNAS(TrueNAS SCALE) NFS的共享作为虚拟机的备份，但是会发现非特权容器都会出现备份失败的情况具体情况如下 123456789101112131415161718192021222324252627INFO: starting new backup job: vzdump 112 --notes-template &#x27;&#123;&#123;guestname&#125;&#125;&#x27; --storage vm_omv --remove 0 --mode snapshot --compress zstd --node serverhubINFO: Starting Backup of VM 112 (lxc)INFO: Backup started at 2023-06-25 14:50:12INFO: status = runningINFO: CT Name: NginxINFO: including mount point rootfs (&#x27;/&#x27;) in backupINFO: mode failure - some volumes do not support snapshotsINFO: trying &#x27;suspend&#x27; mode insteadINFO: backup mode: suspendINFO: ionice priority: 7INFO: CT Name: NginxINFO: including mount point rootfs (&#x27;/&#x27;) in backupINFO: temporary directory is on NFS, disabling xattr and acl support, consider configuring a local tmpdir via /etc/vzdump.confINFO: starting first sync /proc/2982556/root/ to /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmpINFO: first sync finished - transferred 1.29G bytes in 61sINFO: suspending guestINFO: starting final sync /proc/2982556/root/ to /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmpINFO: final sync finished - transferred 1.05M bytes in 1sINFO: resuming guestINFO: guest is online again after 1 secondsINFO: creating vzdump archive &#x27;/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tar.zst&#x27;INFO: tar: /mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp: Cannot open: Permission deniedINFO: tar: Error is not recoverable: exiting nowERROR: Backup of VM 112 failed - command &#x27;set -o pipefail &amp;&amp; lxc-usernsexec -m u:0:100000:65536 -m g:0:100000:65536 -- tar cpf - --totals --one-file-system -p --sparse --numeric-owner --acls --xattrs &#x27;--xattrs-include=user.*&#x27; &#x27;--xattrs-include=security.capability&#x27; &#x27;--warning=no-file-ignored&#x27; &#x27;--warning=no-xattr-write&#x27; --one-file-system &#x27;--warning=no-file-ignored&#x27; &#x27;--directory=/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp&#x27; ./etc/vzdump/pct.conf ./etc/vzdump/pct.fw &#x27;--directory=/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tmp&#x27; --no-anchored &#x27;--exclude=lost+found&#x27; --anchored &#x27;--exclude=./tmp/?*&#x27; &#x27;--exclude=./var/tmp/?*&#x27; &#x27;--exclude=./var/run/?*.pid&#x27; . | zstd --rsyncable &#x27;--threads=1&#x27; &gt;/mnt/pve/vm_omv/dump/vzdump-lxc-112-2023_06_25-14_50_12.tar.dat&#x27; failed: exit code 2INFO: Failed at 2023-06-25 14:51:25INFO: Backup job finished with errorsTASK ERROR: job errors 可以看到报错中提示存储NFS 提示权限限制 原因是因为LXC使用的是非root用户，而我在TrueNAS使用Maproot User会仅限于客户端（PVE）的root用户映射到NFS服务端（TrueNAS）的root用户的权限，导致权限出错。 解决方法 更改备份的临时目录到系统盘就能轻松秒杀，但是我的系统盘是16G傲腾，无法支撑临时备份，所以不行。 1echo &quot;tmpdir: /tmp&quot; &gt;&gt; /etc/vzdump.conf 将Maproot的选项更改到Mapall，即 将“NFS服务器将在客户端访问共享时将所有用户的权限映射为root用户的权限，但仅限于root用户”更改为“NFS服务器将在客户端访问共享时将所有用户的权限映射为root用户的权限。”。让客户端（PVE）非root权限(非特权)的LXC容器也能映射为NFS服务端（TrueNAS）root用户。","categories":[],"tags":[{"name":"ProxmoxVE","slug":"ProxmoxVE","permalink":"https://homelabproject.cc/tags/ProxmoxVE/"}]},{"title":"TrueNAS SCALE（Network UPS Tools） 使用CyberPower UPS 10-20小时 断连","slug":"TrueNAS/TrueNAS CyberPower NUT (Network UPS Tools) 断连","date":"2023-05-21T15:00:00.000Z","updated":"2025-05-23T11:59:08.942Z","comments":true,"path":"2023/05/21/TrueNAS/TrueNAS CyberPower NUT (Network UPS Tools) 断连/","permalink":"https://homelabproject.cc/2023/05/21/TrueNAS/TrueNAS%20CyberPower%20NUT%20(Network%20UPS%20Tools)%20%E6%96%AD%E8%BF%9E/","excerpt":"","text":"在/usr/nut/ups.conf 或者WebUI-系统设置-服务-UPS-Auxiliary Parameters (ups.conf)中加入 1pollonly = &quot;enabled&quot; 保存，重启ups服务即可 图示： 参考来源：https://github.com/networkupstools/nut/issues/1029","categories":[],"tags":[{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"}]}],"categories":[],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"https://homelabproject.cc/tags/Hardware/"},{"name":"TrueNAS","slug":"TrueNAS","permalink":"https://homelabproject.cc/tags/TrueNAS/"},{"name":"RouterOS","slug":"RouterOS","permalink":"https://homelabproject.cc/tags/RouterOS/"},{"name":"Others","slug":"Others","permalink":"https://homelabproject.cc/tags/Others/"},{"name":"ProxmoxVE","slug":"ProxmoxVE","permalink":"https://homelabproject.cc/tags/ProxmoxVE/"}]}